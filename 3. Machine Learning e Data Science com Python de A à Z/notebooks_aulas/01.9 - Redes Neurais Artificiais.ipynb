{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceitos chave:\n",
    "(Supervisionado)\n",
    "\n",
    "- Feedforward\n",
    "    - Pesos aleatórios\n",
    "    - Função sigmoide\n",
    "    - Multicamada\n",
    "    - Camada oculta\n",
    "    - Função soma\n",
    "- Backpropagation\n",
    "    - Erros\n",
    "    - Delta saida\n",
    "    - Delta camada oculta\n",
    "    - Gradiente\n",
    "    - Taxa de aprendizagem\n",
    "    - Momento\n",
    "    - Ajuste dos pesos\n",
    "- Bias\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from yellowbrick.classifier import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Crédito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('credit.pkl', 'rb') as f:\n",
    "    X_credit_train, y_credit_train, X_credit_test, y_credit_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treino:  (1500, 3) (1500,)\n",
      "teste:  (500, 3) (500,)\n"
     ]
    }
   ],
   "source": [
    "print('treino: ',X_credit_train.shape, y_credit_train.shape)\n",
    "print('teste: ',X_credit_test.shape, y_credit_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.77655575\n",
      "Iteration 2, loss = 0.70032366\n",
      "Iteration 3, loss = 0.63307744\n",
      "Iteration 4, loss = 0.57522187\n",
      "Iteration 5, loss = 0.52529099\n",
      "Iteration 6, loss = 0.48265468\n",
      "Iteration 7, loss = 0.44480819\n",
      "Iteration 8, loss = 0.41251943\n",
      "Iteration 9, loss = 0.38381096\n",
      "Iteration 10, loss = 0.35896840\n",
      "Iteration 11, loss = 0.33693090\n",
      "Iteration 12, loss = 0.31732231\n",
      "Iteration 13, loss = 0.29996680\n",
      "Iteration 14, loss = 0.28413651\n",
      "Iteration 15, loss = 0.26994790\n",
      "Iteration 16, loss = 0.25708997\n",
      "Iteration 17, loss = 0.24524163\n",
      "Iteration 18, loss = 0.23452056\n",
      "Iteration 19, loss = 0.22491472\n",
      "Iteration 20, loss = 0.21568064\n",
      "Iteration 21, loss = 0.20738288\n",
      "Iteration 22, loss = 0.19969710\n",
      "Iteration 23, loss = 0.19256935\n",
      "Iteration 24, loss = 0.18600858\n",
      "Iteration 25, loss = 0.17981227\n",
      "Iteration 26, loss = 0.17423344\n",
      "Iteration 27, loss = 0.16881651\n",
      "Iteration 28, loss = 0.16385227\n",
      "Iteration 29, loss = 0.15924761\n",
      "Iteration 30, loss = 0.15499605\n",
      "Iteration 31, loss = 0.15082659\n",
      "Iteration 32, loss = 0.14697793\n",
      "Iteration 33, loss = 0.14333033\n",
      "Iteration 34, loss = 0.13994536\n",
      "Iteration 35, loss = 0.13670627\n",
      "Iteration 36, loss = 0.13364358\n",
      "Iteration 37, loss = 0.13077211\n",
      "Iteration 38, loss = 0.12804408\n",
      "Iteration 39, loss = 0.12538397\n",
      "Iteration 40, loss = 0.12305874\n",
      "Iteration 41, loss = 0.12059613\n",
      "Iteration 42, loss = 0.11829913\n",
      "Iteration 43, loss = 0.11615289\n",
      "Iteration 44, loss = 0.11410606\n",
      "Iteration 45, loss = 0.11216248\n",
      "Iteration 46, loss = 0.11024114\n",
      "Iteration 47, loss = 0.10853569\n",
      "Iteration 48, loss = 0.10666196\n",
      "Iteration 49, loss = 0.10503355\n",
      "Iteration 50, loss = 0.10348442\n",
      "Iteration 51, loss = 0.10180792\n",
      "Iteration 52, loss = 0.10024459\n",
      "Iteration 53, loss = 0.09871929\n",
      "Iteration 54, loss = 0.09730065\n",
      "Iteration 55, loss = 0.09587991\n",
      "Iteration 56, loss = 0.09454532\n",
      "Iteration 57, loss = 0.09322873\n",
      "Iteration 58, loss = 0.09194164\n",
      "Iteration 59, loss = 0.09073182\n",
      "Iteration 60, loss = 0.08957371\n",
      "Iteration 61, loss = 0.08834155\n",
      "Iteration 62, loss = 0.08723986\n",
      "Iteration 63, loss = 0.08616140\n",
      "Iteration 64, loss = 0.08513094\n",
      "Iteration 65, loss = 0.08403731\n",
      "Iteration 66, loss = 0.08299361\n",
      "Iteration 67, loss = 0.08206394\n",
      "Iteration 68, loss = 0.08113013\n",
      "Iteration 69, loss = 0.08018866\n",
      "Iteration 70, loss = 0.07925703\n",
      "Iteration 71, loss = 0.07848303\n",
      "Iteration 72, loss = 0.07760377\n",
      "Iteration 73, loss = 0.07671431\n",
      "Iteration 74, loss = 0.07590658\n",
      "Iteration 75, loss = 0.07515560\n",
      "Iteration 76, loss = 0.07436301\n",
      "Iteration 77, loss = 0.07363753\n",
      "Iteration 78, loss = 0.07292214\n",
      "Iteration 79, loss = 0.07219430\n",
      "Iteration 80, loss = 0.07154528\n",
      "Iteration 81, loss = 0.07087645\n",
      "Iteration 82, loss = 0.07017122\n",
      "Iteration 83, loss = 0.06954313\n",
      "Iteration 84, loss = 0.06891030\n",
      "Iteration 85, loss = 0.06827479\n",
      "Iteration 86, loss = 0.06771471\n",
      "Iteration 87, loss = 0.06709022\n",
      "Iteration 88, loss = 0.06646816\n",
      "Iteration 89, loss = 0.06596769\n",
      "Iteration 90, loss = 0.06544172\n",
      "Iteration 91, loss = 0.06483398\n",
      "Iteration 92, loss = 0.06430166\n",
      "Iteration 93, loss = 0.06379042\n",
      "Iteration 94, loss = 0.06319043\n",
      "Iteration 95, loss = 0.06265399\n",
      "Iteration 96, loss = 0.06216195\n",
      "Iteration 97, loss = 0.06167283\n",
      "Iteration 98, loss = 0.06118760\n",
      "Iteration 99, loss = 0.06066149\n",
      "Iteration 100, loss = 0.06022936\n",
      "Iteration 101, loss = 0.05971868\n",
      "Iteration 102, loss = 0.05927689\n",
      "Iteration 103, loss = 0.05879688\n",
      "Iteration 104, loss = 0.05833507\n",
      "Iteration 105, loss = 0.05794453\n",
      "Iteration 106, loss = 0.05747670\n",
      "Iteration 107, loss = 0.05699945\n",
      "Iteration 108, loss = 0.05660436\n",
      "Iteration 109, loss = 0.05615560\n",
      "Iteration 110, loss = 0.05574385\n",
      "Iteration 111, loss = 0.05536257\n",
      "Iteration 112, loss = 0.05494713\n",
      "Iteration 113, loss = 0.05451410\n",
      "Iteration 114, loss = 0.05415330\n",
      "Iteration 115, loss = 0.05373810\n",
      "Iteration 116, loss = 0.05342923\n",
      "Iteration 117, loss = 0.05307596\n",
      "Iteration 118, loss = 0.05257387\n",
      "Iteration 119, loss = 0.05223455\n",
      "Iteration 120, loss = 0.05187343\n",
      "Iteration 121, loss = 0.05145588\n",
      "Iteration 122, loss = 0.05110563\n",
      "Iteration 123, loss = 0.05072591\n",
      "Iteration 124, loss = 0.05038209\n",
      "Iteration 125, loss = 0.04999353\n",
      "Iteration 126, loss = 0.04964695\n",
      "Iteration 127, loss = 0.04939588\n",
      "Iteration 128, loss = 0.04893228\n",
      "Iteration 129, loss = 0.04860515\n",
      "Iteration 130, loss = 0.04835374\n",
      "Iteration 131, loss = 0.04798376\n",
      "Iteration 132, loss = 0.04760958\n",
      "Iteration 133, loss = 0.04730231\n",
      "Iteration 134, loss = 0.04694120\n",
      "Iteration 135, loss = 0.04660418\n",
      "Iteration 136, loss = 0.04644069\n",
      "Iteration 137, loss = 0.04602260\n",
      "Iteration 138, loss = 0.04570244\n",
      "Iteration 139, loss = 0.04541690\n",
      "Iteration 140, loss = 0.04511905\n",
      "Iteration 141, loss = 0.04486372\n",
      "Iteration 142, loss = 0.04457441\n",
      "Iteration 143, loss = 0.04429411\n",
      "Iteration 144, loss = 0.04403946\n",
      "Iteration 145, loss = 0.04371373\n",
      "Iteration 146, loss = 0.04350984\n",
      "Iteration 147, loss = 0.04322063\n",
      "Iteration 148, loss = 0.04301024\n",
      "Iteration 149, loss = 0.04295620\n",
      "Iteration 150, loss = 0.04254593\n",
      "Iteration 151, loss = 0.04218546\n",
      "Iteration 152, loss = 0.04213412\n",
      "Iteration 153, loss = 0.04172642\n",
      "Iteration 154, loss = 0.04146259\n",
      "Iteration 155, loss = 0.04124116\n",
      "Iteration 156, loss = 0.04101649\n",
      "Iteration 157, loss = 0.04074467\n",
      "Iteration 158, loss = 0.04051519\n",
      "Iteration 159, loss = 0.04028057\n",
      "Iteration 160, loss = 0.04010784\n",
      "Iteration 161, loss = 0.03998095\n",
      "Iteration 162, loss = 0.03970446\n",
      "Iteration 163, loss = 0.03944777\n",
      "Iteration 164, loss = 0.03923633\n",
      "Iteration 165, loss = 0.03894762\n",
      "Iteration 166, loss = 0.03884823\n",
      "Iteration 167, loss = 0.03853243\n",
      "Iteration 168, loss = 0.03838835\n",
      "Iteration 169, loss = 0.03816794\n",
      "Iteration 170, loss = 0.03794683\n",
      "Iteration 171, loss = 0.03774287\n",
      "Iteration 172, loss = 0.03756649\n",
      "Iteration 173, loss = 0.03732130\n",
      "Iteration 174, loss = 0.03713216\n",
      "Iteration 175, loss = 0.03700436\n",
      "Iteration 176, loss = 0.03674634\n",
      "Iteration 177, loss = 0.03655765\n",
      "Iteration 178, loss = 0.03637180\n",
      "Iteration 179, loss = 0.03619208\n",
      "Iteration 180, loss = 0.03597579\n",
      "Iteration 181, loss = 0.03579792\n",
      "Iteration 182, loss = 0.03561331\n",
      "Iteration 183, loss = 0.03548623\n",
      "Iteration 184, loss = 0.03539796\n",
      "Iteration 185, loss = 0.03517301\n",
      "Iteration 186, loss = 0.03492843\n",
      "Iteration 187, loss = 0.03474339\n",
      "Iteration 188, loss = 0.03460753\n",
      "Iteration 189, loss = 0.03445699\n",
      "Iteration 190, loss = 0.03425891\n",
      "Iteration 191, loss = 0.03407392\n",
      "Iteration 192, loss = 0.03393044\n",
      "Iteration 193, loss = 0.03373087\n",
      "Iteration 194, loss = 0.03358833\n",
      "Iteration 195, loss = 0.03347041\n",
      "Iteration 196, loss = 0.03326731\n",
      "Iteration 197, loss = 0.03311187\n",
      "Iteration 198, loss = 0.03298465\n",
      "Iteration 199, loss = 0.03284930\n",
      "Iteration 200, loss = 0.03271835\n",
      "Iteration 201, loss = 0.03251609\n",
      "Iteration 202, loss = 0.03237205\n",
      "Iteration 203, loss = 0.03224731\n",
      "Iteration 204, loss = 0.03209462\n",
      "Iteration 205, loss = 0.03196361\n",
      "Iteration 206, loss = 0.03183081\n",
      "Iteration 207, loss = 0.03163531\n",
      "Iteration 208, loss = 0.03148385\n",
      "Iteration 209, loss = 0.03135895\n",
      "Iteration 210, loss = 0.03142724\n",
      "Iteration 211, loss = 0.03113072\n",
      "Iteration 212, loss = 0.03092576\n",
      "Iteration 213, loss = 0.03076347\n",
      "Iteration 214, loss = 0.03068192\n",
      "Iteration 215, loss = 0.03049397\n",
      "Iteration 216, loss = 0.03039566\n",
      "Iteration 217, loss = 0.03024707\n",
      "Iteration 218, loss = 0.03023729\n",
      "Iteration 219, loss = 0.03000888\n",
      "Iteration 220, loss = 0.03003665\n",
      "Iteration 221, loss = 0.02976072\n",
      "Iteration 222, loss = 0.02961978\n",
      "Iteration 223, loss = 0.02953056\n",
      "Iteration 224, loss = 0.02938559\n",
      "Iteration 225, loss = 0.02925496\n",
      "Iteration 226, loss = 0.02912379\n",
      "Iteration 227, loss = 0.02903391\n",
      "Iteration 228, loss = 0.02889207\n",
      "Iteration 229, loss = 0.02874793\n",
      "Iteration 230, loss = 0.02869302\n",
      "Iteration 231, loss = 0.02849968\n",
      "Iteration 232, loss = 0.02839625\n",
      "Iteration 233, loss = 0.02827432\n",
      "Iteration 234, loss = 0.02823909\n",
      "Iteration 235, loss = 0.02804200\n",
      "Iteration 236, loss = 0.02801570\n",
      "Iteration 237, loss = 0.02786641\n",
      "Iteration 238, loss = 0.02771677\n",
      "Iteration 239, loss = 0.02757473\n",
      "Iteration 240, loss = 0.02749889\n",
      "Iteration 241, loss = 0.02744098\n",
      "Iteration 242, loss = 0.02729888\n",
      "Iteration 243, loss = 0.02719061\n",
      "Iteration 244, loss = 0.02710291\n",
      "Iteration 245, loss = 0.02691865\n",
      "Iteration 246, loss = 0.02684730\n",
      "Iteration 247, loss = 0.02674623\n",
      "Iteration 248, loss = 0.02661404\n",
      "Iteration 249, loss = 0.02653953\n",
      "Iteration 250, loss = 0.02640696\n",
      "Iteration 251, loss = 0.02633175\n",
      "Iteration 252, loss = 0.02619863\n",
      "Iteration 253, loss = 0.02607767\n",
      "Iteration 254, loss = 0.02595755\n",
      "Iteration 255, loss = 0.02594561\n",
      "Iteration 256, loss = 0.02580727\n",
      "Iteration 257, loss = 0.02567800\n",
      "Iteration 258, loss = 0.02562215\n",
      "Iteration 259, loss = 0.02549713\n",
      "Iteration 260, loss = 0.02539939\n",
      "Iteration 261, loss = 0.02533603\n",
      "Iteration 262, loss = 0.02525592\n",
      "Iteration 263, loss = 0.02513041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 264, loss = 0.02504651\n",
      "Iteration 265, loss = 0.02490334\n",
      "Iteration 266, loss = 0.02484639\n",
      "Iteration 267, loss = 0.02477712\n",
      "Iteration 268, loss = 0.02465826\n",
      "Iteration 269, loss = 0.02458047\n",
      "Iteration 270, loss = 0.02447431\n",
      "Iteration 271, loss = 0.02442121\n",
      "Iteration 272, loss = 0.02437501\n",
      "Iteration 273, loss = 0.02424217\n",
      "Iteration 274, loss = 0.02428654\n",
      "Iteration 275, loss = 0.02412672\n",
      "Iteration 276, loss = 0.02395977\n",
      "Iteration 277, loss = 0.02391220\n",
      "Iteration 278, loss = 0.02378615\n",
      "Iteration 279, loss = 0.02371016\n",
      "Iteration 280, loss = 0.02366278\n",
      "Iteration 281, loss = 0.02358331\n",
      "Iteration 282, loss = 0.02354632\n",
      "Iteration 283, loss = 0.02339882\n",
      "Iteration 284, loss = 0.02330447\n",
      "Iteration 285, loss = 0.02326734\n",
      "Iteration 286, loss = 0.02312539\n",
      "Iteration 287, loss = 0.02310106\n",
      "Iteration 288, loss = 0.02298716\n",
      "Iteration 289, loss = 0.02289707\n",
      "Iteration 290, loss = 0.02288905\n",
      "Iteration 291, loss = 0.02278956\n",
      "Iteration 292, loss = 0.02268894\n",
      "Iteration 293, loss = 0.02263788\n",
      "Iteration 294, loss = 0.02256807\n",
      "Iteration 295, loss = 0.02250743\n",
      "Iteration 296, loss = 0.02243412\n",
      "Iteration 297, loss = 0.02230684\n",
      "Iteration 298, loss = 0.02224639\n",
      "Iteration 299, loss = 0.02218264\n",
      "Iteration 300, loss = 0.02213631\n",
      "Iteration 301, loss = 0.02200233\n",
      "Iteration 302, loss = 0.02194724\n",
      "Iteration 303, loss = 0.02185049\n",
      "Iteration 304, loss = 0.02182226\n",
      "Iteration 305, loss = 0.02177531\n",
      "Iteration 306, loss = 0.02170312\n",
      "Iteration 307, loss = 0.02179148\n",
      "Iteration 308, loss = 0.02151730\n",
      "Iteration 309, loss = 0.02143434\n",
      "Iteration 310, loss = 0.02146955\n",
      "Iteration 311, loss = 0.02133993\n",
      "Iteration 312, loss = 0.02129225\n",
      "Iteration 313, loss = 0.02117288\n",
      "Iteration 314, loss = 0.02119755\n",
      "Iteration 315, loss = 0.02115215\n",
      "Iteration 316, loss = 0.02097588\n",
      "Iteration 317, loss = 0.02092467\n",
      "Iteration 318, loss = 0.02082611\n",
      "Iteration 319, loss = 0.02078394\n",
      "Iteration 320, loss = 0.02071503\n",
      "Iteration 321, loss = 0.02071269\n",
      "Iteration 322, loss = 0.02056914\n",
      "Iteration 323, loss = 0.02053383\n",
      "Iteration 324, loss = 0.02046349\n",
      "Iteration 325, loss = 0.02042875\n",
      "Iteration 326, loss = 0.02033478\n",
      "Iteration 327, loss = 0.02022284\n",
      "Iteration 328, loss = 0.02030927\n",
      "Iteration 329, loss = 0.02019799\n",
      "Iteration 330, loss = 0.02012973\n",
      "Iteration 331, loss = 0.02002604\n",
      "Iteration 332, loss = 0.01993641\n",
      "Iteration 333, loss = 0.01988243\n",
      "Iteration 334, loss = 0.01989098\n",
      "Iteration 335, loss = 0.01985347\n",
      "Iteration 336, loss = 0.01972810\n",
      "Iteration 337, loss = 0.01964502\n",
      "Iteration 338, loss = 0.01961561\n",
      "Iteration 339, loss = 0.01955737\n",
      "Iteration 340, loss = 0.01948051\n",
      "Iteration 341, loss = 0.01942851\n",
      "Iteration 342, loss = 0.01946823\n",
      "Iteration 343, loss = 0.01939272\n",
      "Iteration 344, loss = 0.01928866\n",
      "Iteration 345, loss = 0.01922114\n",
      "Iteration 346, loss = 0.01915107\n",
      "Iteration 347, loss = 0.01907048\n",
      "Iteration 348, loss = 0.01910158\n",
      "Iteration 349, loss = 0.01906580\n",
      "Iteration 350, loss = 0.01892442\n",
      "Iteration 351, loss = 0.01882833\n",
      "Iteration 352, loss = 0.01882738\n",
      "Iteration 353, loss = 0.01884809\n",
      "Iteration 354, loss = 0.01872347\n",
      "Iteration 355, loss = 0.01864477\n",
      "Iteration 356, loss = 0.01861032\n",
      "Iteration 357, loss = 0.01851699\n",
      "Iteration 358, loss = 0.01849241\n",
      "Iteration 359, loss = 0.01846464\n",
      "Iteration 360, loss = 0.01836875\n",
      "Iteration 361, loss = 0.01837565\n",
      "Iteration 362, loss = 0.01836633\n",
      "Iteration 363, loss = 0.01820401\n",
      "Iteration 364, loss = 0.01814445\n",
      "Iteration 365, loss = 0.01812241\n",
      "Iteration 366, loss = 0.01811801\n",
      "Iteration 367, loss = 0.01806991\n",
      "Iteration 368, loss = 0.01805762\n",
      "Iteration 369, loss = 0.01789990\n",
      "Iteration 370, loss = 0.01796196\n",
      "Iteration 371, loss = 0.01783012\n",
      "Iteration 372, loss = 0.01774323\n",
      "Iteration 373, loss = 0.01777032\n",
      "Iteration 374, loss = 0.01768246\n",
      "Iteration 375, loss = 0.01759645\n",
      "Iteration 376, loss = 0.01756301\n",
      "Iteration 377, loss = 0.01747932\n",
      "Iteration 378, loss = 0.01744271\n",
      "Iteration 379, loss = 0.01752042\n",
      "Iteration 380, loss = 0.01745145\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=1000, verbose=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural_credit = MLPClassifier(max_iter=1000, verbose=True, tol=0.0001)\n",
    "rede_neural_credit.fit(X_credit_train, y_credit_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = rede_neural_credit.predict(X_credit_test)\n",
    "previsoes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_credit_test, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(rede_neural_credit)\n",
    "cm.fit(X_credit_train, y_credit_train)\n",
    "cm.score(X_credit_test, y_credit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       0.97      0.97      0.97        64\n",
      "\n",
      "    accuracy                           0.99       500\n",
      "   macro avg       0.98      0.98      0.98       500\n",
      "weighted avg       0.99      0.99      0.99       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_credit_test,previsoes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('census.pkl', 'rb') as f:\n",
    "    X_census_train, y_census_train, X_census_test, y_census_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treino:  (27676, 108) (27676,)\n",
      "teste:  (4885, 108) (4885,)\n"
     ]
    }
   ],
   "source": [
    "print('treino: ',X_census_train.shape, y_census_train.shape)\n",
    "print('teste: ',X_census_test.shape, y_census_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.37742217\n",
      "Iteration 2, loss = 0.32502043\n",
      "Iteration 3, loss = 0.31522697\n",
      "Iteration 4, loss = 0.30821679\n",
      "Iteration 5, loss = 0.30333031\n",
      "Iteration 6, loss = 0.30006488\n",
      "Iteration 7, loss = 0.29696375\n",
      "Iteration 8, loss = 0.29445465\n",
      "Iteration 9, loss = 0.29189660\n",
      "Iteration 10, loss = 0.29024070\n",
      "Iteration 11, loss = 0.28834612\n",
      "Iteration 12, loss = 0.28621494\n",
      "Iteration 13, loss = 0.28501447\n",
      "Iteration 14, loss = 0.28370791\n",
      "Iteration 15, loss = 0.28216022\n",
      "Iteration 16, loss = 0.28097371\n",
      "Iteration 17, loss = 0.27956183\n",
      "Iteration 18, loss = 0.27932384\n",
      "Iteration 19, loss = 0.27704158\n",
      "Iteration 20, loss = 0.27625464\n",
      "Iteration 21, loss = 0.27462369\n",
      "Iteration 22, loss = 0.27444256\n",
      "Iteration 23, loss = 0.27328094\n",
      "Iteration 24, loss = 0.27221796\n",
      "Iteration 25, loss = 0.27168752\n",
      "Iteration 26, loss = 0.27090633\n",
      "Iteration 27, loss = 0.26996652\n",
      "Iteration 28, loss = 0.26957590\n",
      "Iteration 29, loss = 0.26780137\n",
      "Iteration 30, loss = 0.26744006\n",
      "Iteration 31, loss = 0.26648854\n",
      "Iteration 32, loss = 0.26629229\n",
      "Iteration 33, loss = 0.26445006\n",
      "Iteration 34, loss = 0.26494144\n",
      "Iteration 35, loss = 0.26442403\n",
      "Iteration 36, loss = 0.26268058\n",
      "Iteration 37, loss = 0.26266675\n",
      "Iteration 38, loss = 0.26137088\n",
      "Iteration 39, loss = 0.26095607\n",
      "Iteration 40, loss = 0.25986495\n",
      "Iteration 41, loss = 0.25989807\n",
      "Iteration 42, loss = 0.25954272\n",
      "Iteration 43, loss = 0.25814979\n",
      "Iteration 44, loss = 0.25704941\n",
      "Iteration 45, loss = 0.25665775\n",
      "Iteration 46, loss = 0.25641400\n",
      "Iteration 47, loss = 0.25624869\n",
      "Iteration 48, loss = 0.25573827\n",
      "Iteration 49, loss = 0.25531628\n",
      "Iteration 50, loss = 0.25468412\n",
      "Iteration 51, loss = 0.25414211\n",
      "Iteration 52, loss = 0.25323182\n",
      "Iteration 53, loss = 0.25282085\n",
      "Iteration 54, loss = 0.25185207\n",
      "Iteration 55, loss = 0.25191513\n",
      "Iteration 56, loss = 0.25202354\n",
      "Iteration 57, loss = 0.25140235\n",
      "Iteration 58, loss = 0.25051629\n",
      "Iteration 59, loss = 0.24952176\n",
      "Iteration 60, loss = 0.24943886\n",
      "Iteration 61, loss = 0.24925653\n",
      "Iteration 62, loss = 0.24880773\n",
      "Iteration 63, loss = 0.24819250\n",
      "Iteration 64, loss = 0.24750811\n",
      "Iteration 65, loss = 0.24762952\n",
      "Iteration 66, loss = 0.24649603\n",
      "Iteration 67, loss = 0.24700307\n",
      "Iteration 68, loss = 0.24637583\n",
      "Iteration 69, loss = 0.24550362\n",
      "Iteration 70, loss = 0.24470819\n",
      "Iteration 71, loss = 0.24485027\n",
      "Iteration 72, loss = 0.24356255\n",
      "Iteration 73, loss = 0.24300962\n",
      "Iteration 74, loss = 0.24307399\n",
      "Iteration 75, loss = 0.24292524\n",
      "Iteration 76, loss = 0.24292047\n",
      "Iteration 77, loss = 0.24317317\n",
      "Iteration 78, loss = 0.24222761\n",
      "Iteration 79, loss = 0.24212159\n",
      "Iteration 80, loss = 0.24164500\n",
      "Iteration 81, loss = 0.24057528\n",
      "Iteration 82, loss = 0.23978136\n",
      "Iteration 83, loss = 0.23932142\n",
      "Iteration 84, loss = 0.23994449\n",
      "Iteration 85, loss = 0.23930782\n",
      "Iteration 86, loss = 0.23852439\n",
      "Iteration 87, loss = 0.23837349\n",
      "Iteration 88, loss = 0.23822148\n",
      "Iteration 89, loss = 0.23660730\n",
      "Iteration 90, loss = 0.23883111\n",
      "Iteration 91, loss = 0.23693947\n",
      "Iteration 92, loss = 0.23667448\n",
      "Iteration 93, loss = 0.23566050\n",
      "Iteration 94, loss = 0.23566220\n",
      "Iteration 95, loss = 0.23578089\n",
      "Iteration 96, loss = 0.23455831\n",
      "Iteration 97, loss = 0.23439664\n",
      "Iteration 98, loss = 0.23420567\n",
      "Iteration 99, loss = 0.23452743\n",
      "Iteration 100, loss = 0.23387121\n",
      "Iteration 101, loss = 0.23367706\n",
      "Iteration 102, loss = 0.23307251\n",
      "Iteration 103, loss = 0.23275513\n",
      "Iteration 104, loss = 0.23223378\n",
      "Iteration 105, loss = 0.23211848\n",
      "Iteration 106, loss = 0.23190406\n",
      "Iteration 107, loss = 0.23096361\n",
      "Iteration 108, loss = 0.23305448\n",
      "Iteration 109, loss = 0.23060281\n",
      "Iteration 110, loss = 0.23087283\n",
      "Iteration 111, loss = 0.23017134\n",
      "Iteration 112, loss = 0.23021842\n",
      "Iteration 113, loss = 0.22984741\n",
      "Iteration 114, loss = 0.23031620\n",
      "Iteration 115, loss = 0.22964799\n",
      "Iteration 116, loss = 0.22871230\n",
      "Iteration 117, loss = 0.22861422\n",
      "Iteration 118, loss = 0.22788568\n",
      "Iteration 119, loss = 0.22830764\n",
      "Iteration 120, loss = 0.22794449\n",
      "Iteration 121, loss = 0.22727003\n",
      "Iteration 122, loss = 0.22669098\n",
      "Iteration 123, loss = 0.22761003\n",
      "Iteration 124, loss = 0.22714800\n",
      "Iteration 125, loss = 0.22641663\n",
      "Iteration 126, loss = 0.22525610\n",
      "Iteration 127, loss = 0.22612021\n",
      "Iteration 128, loss = 0.22449140\n",
      "Iteration 129, loss = 0.22535258\n",
      "Iteration 130, loss = 0.22462438\n",
      "Iteration 131, loss = 0.22489755\n",
      "Iteration 132, loss = 0.22447139\n",
      "Iteration 133, loss = 0.22439133\n",
      "Iteration 134, loss = 0.22437015\n",
      "Iteration 135, loss = 0.22315399\n",
      "Iteration 136, loss = 0.22371435\n",
      "Iteration 137, loss = 0.22260489\n",
      "Iteration 138, loss = 0.22220184\n",
      "Iteration 139, loss = 0.22241133\n",
      "Iteration 140, loss = 0.22315322\n",
      "Iteration 141, loss = 0.22111727\n",
      "Iteration 142, loss = 0.22148089\n",
      "Iteration 143, loss = 0.22164043\n",
      "Iteration 144, loss = 0.22245487\n",
      "Iteration 145, loss = 0.22154157\n",
      "Iteration 146, loss = 0.22060895\n",
      "Iteration 147, loss = 0.21971645\n",
      "Iteration 148, loss = 0.22023377\n",
      "Iteration 149, loss = 0.22088545\n",
      "Iteration 150, loss = 0.22027551\n",
      "Iteration 151, loss = 0.21827859\n",
      "Iteration 152, loss = 0.21877864\n",
      "Iteration 153, loss = 0.21915438\n",
      "Iteration 154, loss = 0.22038105\n",
      "Iteration 155, loss = 0.21773732\n",
      "Iteration 156, loss = 0.21764552\n",
      "Iteration 157, loss = 0.21666432\n",
      "Iteration 158, loss = 0.21745627\n",
      "Iteration 159, loss = 0.21765008\n",
      "Iteration 160, loss = 0.21719657\n",
      "Iteration 161, loss = 0.21671373\n",
      "Iteration 162, loss = 0.21671816\n",
      "Iteration 163, loss = 0.21683593\n",
      "Iteration 164, loss = 0.21632009\n",
      "Iteration 165, loss = 0.21803957\n",
      "Iteration 166, loss = 0.21580552\n",
      "Iteration 167, loss = 0.21540465\n",
      "Iteration 168, loss = 0.21520133\n",
      "Iteration 169, loss = 0.21592627\n",
      "Iteration 170, loss = 0.21518548\n",
      "Iteration 171, loss = 0.21509348\n",
      "Iteration 172, loss = 0.21454993\n",
      "Iteration 173, loss = 0.21396512\n",
      "Iteration 174, loss = 0.21464880\n",
      "Iteration 175, loss = 0.21463760\n",
      "Iteration 176, loss = 0.21391103\n",
      "Iteration 177, loss = 0.21422553\n",
      "Iteration 178, loss = 0.21456676\n",
      "Iteration 179, loss = 0.21435658\n",
      "Iteration 180, loss = 0.21222894\n",
      "Iteration 181, loss = 0.21209511\n",
      "Iteration 182, loss = 0.21182284\n",
      "Iteration 183, loss = 0.21234527\n",
      "Iteration 184, loss = 0.21149594\n",
      "Iteration 185, loss = 0.21250363\n",
      "Iteration 186, loss = 0.21307562\n",
      "Iteration 187, loss = 0.21122045\n",
      "Iteration 188, loss = 0.21056300\n",
      "Iteration 189, loss = 0.21134139\n",
      "Iteration 190, loss = 0.21150503\n",
      "Iteration 191, loss = 0.21019493\n",
      "Iteration 192, loss = 0.21072796\n",
      "Iteration 193, loss = 0.21052264\n",
      "Iteration 194, loss = 0.21131927\n",
      "Iteration 195, loss = 0.20913742\n",
      "Iteration 196, loss = 0.20959943\n",
      "Iteration 197, loss = 0.21041643\n",
      "Iteration 198, loss = 0.21118779\n",
      "Iteration 199, loss = 0.20962507\n",
      "Iteration 200, loss = 0.21114084\n",
      "Iteration 201, loss = 0.20836722\n",
      "Iteration 202, loss = 0.20964112\n",
      "Iteration 203, loss = 0.20810216\n",
      "Iteration 204, loss = 0.20844655\n",
      "Iteration 205, loss = 0.20886166\n",
      "Iteration 206, loss = 0.20747772\n",
      "Iteration 207, loss = 0.20782659\n",
      "Iteration 208, loss = 0.20745231\n",
      "Iteration 209, loss = 0.20715982\n",
      "Iteration 210, loss = 0.20686012\n",
      "Iteration 211, loss = 0.20753793\n",
      "Iteration 212, loss = 0.20654892\n",
      "Iteration 213, loss = 0.20674441\n",
      "Iteration 214, loss = 0.20683429\n",
      "Iteration 215, loss = 0.20608235\n",
      "Iteration 216, loss = 0.20642756\n",
      "Iteration 217, loss = 0.20757973\n",
      "Iteration 218, loss = 0.20644860\n",
      "Iteration 219, loss = 0.20715694\n",
      "Iteration 220, loss = 0.20435739\n",
      "Iteration 221, loss = 0.20548101\n",
      "Iteration 222, loss = 0.20428460\n",
      "Iteration 223, loss = 0.20429204\n",
      "Iteration 224, loss = 0.20466854\n",
      "Iteration 225, loss = 0.20378584\n",
      "Iteration 226, loss = 0.20525952\n",
      "Iteration 227, loss = 0.20544990\n",
      "Iteration 228, loss = 0.20447259\n",
      "Iteration 229, loss = 0.20414382\n",
      "Iteration 230, loss = 0.20294621\n",
      "Iteration 231, loss = 0.20400124\n",
      "Iteration 232, loss = 0.20388842\n",
      "Iteration 233, loss = 0.20445560\n",
      "Iteration 234, loss = 0.20377971\n",
      "Iteration 235, loss = 0.20393651\n",
      "Iteration 236, loss = 0.20232214\n",
      "Iteration 237, loss = 0.20354333\n",
      "Iteration 238, loss = 0.20130903\n",
      "Iteration 239, loss = 0.20196485\n",
      "Iteration 240, loss = 0.20250289\n",
      "Iteration 241, loss = 0.20170435\n",
      "Iteration 242, loss = 0.20238464\n",
      "Iteration 243, loss = 0.20291287\n",
      "Iteration 244, loss = 0.20141544\n",
      "Iteration 245, loss = 0.20166316\n",
      "Iteration 246, loss = 0.20212509\n",
      "Iteration 247, loss = 0.20153737\n",
      "Iteration 248, loss = 0.20037971\n",
      "Iteration 249, loss = 0.20125723\n",
      "Iteration 250, loss = 0.19985332\n",
      "Iteration 251, loss = 0.19964219\n",
      "Iteration 252, loss = 0.20075247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.20087266\n",
      "Iteration 254, loss = 0.19877593\n",
      "Iteration 255, loss = 0.20142366\n",
      "Iteration 256, loss = 0.20090901\n",
      "Iteration 257, loss = 0.19993140\n",
      "Iteration 258, loss = 0.19999786\n",
      "Iteration 259, loss = 0.19965166\n",
      "Iteration 260, loss = 0.19908408\n",
      "Iteration 261, loss = 0.19966533\n",
      "Iteration 262, loss = 0.19801484\n",
      "Iteration 263, loss = 0.20027893\n",
      "Iteration 264, loss = 0.19858312\n",
      "Iteration 265, loss = 0.19986238\n",
      "Iteration 266, loss = 0.19962771\n",
      "Iteration 267, loss = 0.19882366\n",
      "Iteration 268, loss = 0.20031203\n",
      "Iteration 269, loss = 0.19917569\n",
      "Iteration 270, loss = 0.19824431\n",
      "Iteration 271, loss = 0.19688310\n",
      "Iteration 272, loss = 0.19857350\n",
      "Iteration 273, loss = 0.19721230\n",
      "Iteration 274, loss = 0.19724338\n",
      "Iteration 275, loss = 0.19846370\n",
      "Iteration 276, loss = 0.19734986\n",
      "Iteration 277, loss = 0.19586438\n",
      "Iteration 278, loss = 0.19732318\n",
      "Iteration 279, loss = 0.19651557\n",
      "Iteration 280, loss = 0.19669004\n",
      "Iteration 281, loss = 0.19729627\n",
      "Iteration 282, loss = 0.19775027\n",
      "Iteration 283, loss = 0.19630270\n",
      "Iteration 284, loss = 0.19609540\n",
      "Iteration 285, loss = 0.19592306\n",
      "Iteration 286, loss = 0.19735479\n",
      "Iteration 287, loss = 0.19548347\n",
      "Iteration 288, loss = 0.19650720\n",
      "Iteration 289, loss = 0.19538346\n",
      "Iteration 290, loss = 0.19614381\n",
      "Iteration 291, loss = 0.19476360\n",
      "Iteration 292, loss = 0.19404070\n",
      "Iteration 293, loss = 0.19508955\n",
      "Iteration 294, loss = 0.19522019\n",
      "Iteration 295, loss = 0.19632410\n",
      "Iteration 296, loss = 0.19473393\n",
      "Iteration 297, loss = 0.19475825\n",
      "Iteration 298, loss = 0.19406055\n",
      "Iteration 299, loss = 0.19395919\n",
      "Iteration 300, loss = 0.19570318\n",
      "Iteration 301, loss = 0.19340191\n",
      "Iteration 302, loss = 0.19498164\n",
      "Iteration 303, loss = 0.19480666\n",
      "Iteration 304, loss = 0.19502672\n",
      "Iteration 305, loss = 0.19335210\n",
      "Iteration 306, loss = 0.19320213\n",
      "Iteration 307, loss = 0.19343184\n",
      "Iteration 308, loss = 0.19369898\n",
      "Iteration 309, loss = 0.19410805\n",
      "Iteration 310, loss = 0.19568205\n",
      "Iteration 311, loss = 0.19440012\n",
      "Iteration 312, loss = 0.19224346\n",
      "Iteration 313, loss = 0.19122913\n",
      "Iteration 314, loss = 0.19242003\n",
      "Iteration 315, loss = 0.19189366\n",
      "Iteration 316, loss = 0.19292731\n",
      "Iteration 317, loss = 0.19259820\n",
      "Iteration 318, loss = 0.19245795\n",
      "Iteration 319, loss = 0.19167427\n",
      "Iteration 320, loss = 0.19209372\n",
      "Iteration 321, loss = 0.19250603\n",
      "Iteration 322, loss = 0.19228548\n",
      "Iteration 323, loss = 0.19271313\n",
      "Iteration 324, loss = 0.19228861\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=1000, verbose=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural_census = MLPClassifier(max_iter=1000, verbose=True)\n",
    "rede_neural_census.fit(X_census_train, y_census_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4885,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = rede_neural_census.predict(X_census_test)\n",
    "previsoes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8325486182190379"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(rede_neural_census)\n",
    "cm.fit(X_census_train, y_census_train)\n",
    "cm.score(X_census_test, y_census_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.90      0.89      3693\n",
      "        >50K       0.67      0.63      0.65      1192\n",
      "\n",
      "    accuracy                           0.83      4885\n",
      "   macro avg       0.77      0.76      0.77      4885\n",
      "weighted avg       0.83      0.83      0.83      4885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_census_test,previsoes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
