{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceitos chave:\n",
    "(Supervisionado)\n",
    "\n",
    "- Feedforward\n",
    "    - Pesos aleatórios\n",
    "    - Função sigmoide\n",
    "    - Multicamada\n",
    "    - Camada oculta\n",
    "    - Função soma\n",
    "- Backpropagation\n",
    "    - Erros\n",
    "    - Delta saida\n",
    "    - Delta camada oculta\n",
    "    - Gradiente\n",
    "    - Taxa de aprendizagem\n",
    "    - Momento\n",
    "    - Ajuste dos pesos\n",
    "- Bias\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from yellowbrick.classifier import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Crédito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('credit.pkl', 'rb') as f:\n",
    "    X_credit_train, y_credit_train, X_credit_test, y_credit_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treino:  (1500, 3) (1500,)\n",
      "teste:  (500, 3) (500,)\n"
     ]
    }
   ],
   "source": [
    "print('treino: ',X_credit_train.shape, y_credit_train.shape)\n",
    "print('teste: ',X_credit_test.shape, y_credit_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.77655575\n",
      "Iteration 2, loss = 0.70032366\n",
      "Iteration 3, loss = 0.63307744\n",
      "Iteration 4, loss = 0.57522187\n",
      "Iteration 5, loss = 0.52529099\n",
      "Iteration 6, loss = 0.48265468\n",
      "Iteration 7, loss = 0.44480819\n",
      "Iteration 8, loss = 0.41251943\n",
      "Iteration 9, loss = 0.38381096\n",
      "Iteration 10, loss = 0.35896840\n",
      "Iteration 11, loss = 0.33693090\n",
      "Iteration 12, loss = 0.31732231\n",
      "Iteration 13, loss = 0.29996680\n",
      "Iteration 14, loss = 0.28413651\n",
      "Iteration 15, loss = 0.26994790\n",
      "Iteration 16, loss = 0.25708997\n",
      "Iteration 17, loss = 0.24524163\n",
      "Iteration 18, loss = 0.23452056\n",
      "Iteration 19, loss = 0.22491472\n",
      "Iteration 20, loss = 0.21568064\n",
      "Iteration 21, loss = 0.20738288\n",
      "Iteration 22, loss = 0.19969710\n",
      "Iteration 23, loss = 0.19256935\n",
      "Iteration 24, loss = 0.18600858\n",
      "Iteration 25, loss = 0.17981227\n",
      "Iteration 26, loss = 0.17423344\n",
      "Iteration 27, loss = 0.16881651\n",
      "Iteration 28, loss = 0.16385227\n",
      "Iteration 29, loss = 0.15924761\n",
      "Iteration 30, loss = 0.15499605\n",
      "Iteration 31, loss = 0.15082659\n",
      "Iteration 32, loss = 0.14697793\n",
      "Iteration 33, loss = 0.14333033\n",
      "Iteration 34, loss = 0.13994536\n",
      "Iteration 35, loss = 0.13670627\n",
      "Iteration 36, loss = 0.13364358\n",
      "Iteration 37, loss = 0.13077211\n",
      "Iteration 38, loss = 0.12804408\n",
      "Iteration 39, loss = 0.12538397\n",
      "Iteration 40, loss = 0.12305874\n",
      "Iteration 41, loss = 0.12059613\n",
      "Iteration 42, loss = 0.11829913\n",
      "Iteration 43, loss = 0.11615289\n",
      "Iteration 44, loss = 0.11410606\n",
      "Iteration 45, loss = 0.11216248\n",
      "Iteration 46, loss = 0.11024114\n",
      "Iteration 47, loss = 0.10853569\n",
      "Iteration 48, loss = 0.10666196\n",
      "Iteration 49, loss = 0.10503355\n",
      "Iteration 50, loss = 0.10348442\n",
      "Iteration 51, loss = 0.10180792\n",
      "Iteration 52, loss = 0.10024459\n",
      "Iteration 53, loss = 0.09871929\n",
      "Iteration 54, loss = 0.09730065\n",
      "Iteration 55, loss = 0.09587991\n",
      "Iteration 56, loss = 0.09454532\n",
      "Iteration 57, loss = 0.09322873\n",
      "Iteration 58, loss = 0.09194164\n",
      "Iteration 59, loss = 0.09073182\n",
      "Iteration 60, loss = 0.08957371\n",
      "Iteration 61, loss = 0.08834155\n",
      "Iteration 62, loss = 0.08723986\n",
      "Iteration 63, loss = 0.08616140\n",
      "Iteration 64, loss = 0.08513094\n",
      "Iteration 65, loss = 0.08403731\n",
      "Iteration 66, loss = 0.08299361\n",
      "Iteration 67, loss = 0.08206394\n",
      "Iteration 68, loss = 0.08113013\n",
      "Iteration 69, loss = 0.08018866\n",
      "Iteration 70, loss = 0.07925703\n",
      "Iteration 71, loss = 0.07848303\n",
      "Iteration 72, loss = 0.07760377\n",
      "Iteration 73, loss = 0.07671431\n",
      "Iteration 74, loss = 0.07590658\n",
      "Iteration 75, loss = 0.07515560\n",
      "Iteration 76, loss = 0.07436301\n",
      "Iteration 77, loss = 0.07363753\n",
      "Iteration 78, loss = 0.07292214\n",
      "Iteration 79, loss = 0.07219430\n",
      "Iteration 80, loss = 0.07154528\n",
      "Iteration 81, loss = 0.07087645\n",
      "Iteration 82, loss = 0.07017122\n",
      "Iteration 83, loss = 0.06954313\n",
      "Iteration 84, loss = 0.06891030\n",
      "Iteration 85, loss = 0.06827479\n",
      "Iteration 86, loss = 0.06771471\n",
      "Iteration 87, loss = 0.06709022\n",
      "Iteration 88, loss = 0.06646816\n",
      "Iteration 89, loss = 0.06596769\n",
      "Iteration 90, loss = 0.06544172\n",
      "Iteration 91, loss = 0.06483398\n",
      "Iteration 92, loss = 0.06430166\n",
      "Iteration 93, loss = 0.06379042\n",
      "Iteration 94, loss = 0.06319043\n",
      "Iteration 95, loss = 0.06265399\n",
      "Iteration 96, loss = 0.06216195\n",
      "Iteration 97, loss = 0.06167283\n",
      "Iteration 98, loss = 0.06118760\n",
      "Iteration 99, loss = 0.06066149\n",
      "Iteration 100, loss = 0.06022936\n",
      "Iteration 101, loss = 0.05971868\n",
      "Iteration 102, loss = 0.05927689\n",
      "Iteration 103, loss = 0.05879688\n",
      "Iteration 104, loss = 0.05833507\n",
      "Iteration 105, loss = 0.05794453\n",
      "Iteration 106, loss = 0.05747670\n",
      "Iteration 107, loss = 0.05699945\n",
      "Iteration 108, loss = 0.05660436\n",
      "Iteration 109, loss = 0.05615560\n",
      "Iteration 110, loss = 0.05574385\n",
      "Iteration 111, loss = 0.05536257\n",
      "Iteration 112, loss = 0.05494713\n",
      "Iteration 113, loss = 0.05451410\n",
      "Iteration 114, loss = 0.05415330\n",
      "Iteration 115, loss = 0.05373810\n",
      "Iteration 116, loss = 0.05342923\n",
      "Iteration 117, loss = 0.05307596\n",
      "Iteration 118, loss = 0.05257387\n",
      "Iteration 119, loss = 0.05223455\n",
      "Iteration 120, loss = 0.05187343\n",
      "Iteration 121, loss = 0.05145588\n",
      "Iteration 122, loss = 0.05110563\n",
      "Iteration 123, loss = 0.05072591\n",
      "Iteration 124, loss = 0.05038209\n",
      "Iteration 125, loss = 0.04999353\n",
      "Iteration 126, loss = 0.04964695\n",
      "Iteration 127, loss = 0.04939588\n",
      "Iteration 128, loss = 0.04893228\n",
      "Iteration 129, loss = 0.04860515\n",
      "Iteration 130, loss = 0.04835374\n",
      "Iteration 131, loss = 0.04798376\n",
      "Iteration 132, loss = 0.04760958\n",
      "Iteration 133, loss = 0.04730231\n",
      "Iteration 134, loss = 0.04694120\n",
      "Iteration 135, loss = 0.04660418\n",
      "Iteration 136, loss = 0.04644069\n",
      "Iteration 137, loss = 0.04602260\n",
      "Iteration 138, loss = 0.04570244\n",
      "Iteration 139, loss = 0.04541690\n",
      "Iteration 140, loss = 0.04511905\n",
      "Iteration 141, loss = 0.04486372\n",
      "Iteration 142, loss = 0.04457441\n",
      "Iteration 143, loss = 0.04429411\n",
      "Iteration 144, loss = 0.04403946\n",
      "Iteration 145, loss = 0.04371373\n",
      "Iteration 146, loss = 0.04350984\n",
      "Iteration 147, loss = 0.04322063\n",
      "Iteration 148, loss = 0.04301024\n",
      "Iteration 149, loss = 0.04295620\n",
      "Iteration 150, loss = 0.04254593\n",
      "Iteration 151, loss = 0.04218546\n",
      "Iteration 152, loss = 0.04213412\n",
      "Iteration 153, loss = 0.04172642\n",
      "Iteration 154, loss = 0.04146259\n",
      "Iteration 155, loss = 0.04124116\n",
      "Iteration 156, loss = 0.04101649\n",
      "Iteration 157, loss = 0.04074467\n",
      "Iteration 158, loss = 0.04051519\n",
      "Iteration 159, loss = 0.04028057\n",
      "Iteration 160, loss = 0.04010784\n",
      "Iteration 161, loss = 0.03998095\n",
      "Iteration 162, loss = 0.03970446\n",
      "Iteration 163, loss = 0.03944777\n",
      "Iteration 164, loss = 0.03923633\n",
      "Iteration 165, loss = 0.03894762\n",
      "Iteration 166, loss = 0.03884823\n",
      "Iteration 167, loss = 0.03853243\n",
      "Iteration 168, loss = 0.03838835\n",
      "Iteration 169, loss = 0.03816794\n",
      "Iteration 170, loss = 0.03794683\n",
      "Iteration 171, loss = 0.03774287\n",
      "Iteration 172, loss = 0.03756649\n",
      "Iteration 173, loss = 0.03732130\n",
      "Iteration 174, loss = 0.03713216\n",
      "Iteration 175, loss = 0.03700436\n",
      "Iteration 176, loss = 0.03674634\n",
      "Iteration 177, loss = 0.03655765\n",
      "Iteration 178, loss = 0.03637180\n",
      "Iteration 179, loss = 0.03619208\n",
      "Iteration 180, loss = 0.03597579\n",
      "Iteration 181, loss = 0.03579792\n",
      "Iteration 182, loss = 0.03561331\n",
      "Iteration 183, loss = 0.03548623\n",
      "Iteration 184, loss = 0.03539796\n",
      "Iteration 185, loss = 0.03517301\n",
      "Iteration 186, loss = 0.03492843\n",
      "Iteration 187, loss = 0.03474339\n",
      "Iteration 188, loss = 0.03460753\n",
      "Iteration 189, loss = 0.03445699\n",
      "Iteration 190, loss = 0.03425891\n",
      "Iteration 191, loss = 0.03407392\n",
      "Iteration 192, loss = 0.03393044\n",
      "Iteration 193, loss = 0.03373087\n",
      "Iteration 194, loss = 0.03358833\n",
      "Iteration 195, loss = 0.03347041\n",
      "Iteration 196, loss = 0.03326731\n",
      "Iteration 197, loss = 0.03311187\n",
      "Iteration 198, loss = 0.03298465\n",
      "Iteration 199, loss = 0.03284930\n",
      "Iteration 200, loss = 0.03271835\n",
      "Iteration 201, loss = 0.03251609\n",
      "Iteration 202, loss = 0.03237205\n",
      "Iteration 203, loss = 0.03224731\n",
      "Iteration 204, loss = 0.03209462\n",
      "Iteration 205, loss = 0.03196361\n",
      "Iteration 206, loss = 0.03183081\n",
      "Iteration 207, loss = 0.03163531\n",
      "Iteration 208, loss = 0.03148385\n",
      "Iteration 209, loss = 0.03135895\n",
      "Iteration 210, loss = 0.03142724\n",
      "Iteration 211, loss = 0.03113072\n",
      "Iteration 212, loss = 0.03092576\n",
      "Iteration 213, loss = 0.03076347\n",
      "Iteration 214, loss = 0.03068192\n",
      "Iteration 215, loss = 0.03049397\n",
      "Iteration 216, loss = 0.03039566\n",
      "Iteration 217, loss = 0.03024707\n",
      "Iteration 218, loss = 0.03023729\n",
      "Iteration 219, loss = 0.03000888\n",
      "Iteration 220, loss = 0.03003665\n",
      "Iteration 221, loss = 0.02976072\n",
      "Iteration 222, loss = 0.02961978\n",
      "Iteration 223, loss = 0.02953056\n",
      "Iteration 224, loss = 0.02938559\n",
      "Iteration 225, loss = 0.02925496\n",
      "Iteration 226, loss = 0.02912379\n",
      "Iteration 227, loss = 0.02903391\n",
      "Iteration 228, loss = 0.02889207\n",
      "Iteration 229, loss = 0.02874793\n",
      "Iteration 230, loss = 0.02869302\n",
      "Iteration 231, loss = 0.02849968\n",
      "Iteration 232, loss = 0.02839625\n",
      "Iteration 233, loss = 0.02827432\n",
      "Iteration 234, loss = 0.02823909\n",
      "Iteration 235, loss = 0.02804200\n",
      "Iteration 236, loss = 0.02801570\n",
      "Iteration 237, loss = 0.02786641\n",
      "Iteration 238, loss = 0.02771677\n",
      "Iteration 239, loss = 0.02757473\n",
      "Iteration 240, loss = 0.02749889\n",
      "Iteration 241, loss = 0.02744098\n",
      "Iteration 242, loss = 0.02729888\n",
      "Iteration 243, loss = 0.02719061\n",
      "Iteration 244, loss = 0.02710291\n",
      "Iteration 245, loss = 0.02691865\n",
      "Iteration 246, loss = 0.02684730\n",
      "Iteration 247, loss = 0.02674623\n",
      "Iteration 248, loss = 0.02661404\n",
      "Iteration 249, loss = 0.02653953\n",
      "Iteration 250, loss = 0.02640696\n",
      "Iteration 251, loss = 0.02633175\n",
      "Iteration 252, loss = 0.02619863\n",
      "Iteration 253, loss = 0.02607767\n",
      "Iteration 254, loss = 0.02595755\n",
      "Iteration 255, loss = 0.02594561\n",
      "Iteration 256, loss = 0.02580727\n",
      "Iteration 257, loss = 0.02567800\n",
      "Iteration 258, loss = 0.02562215\n",
      "Iteration 259, loss = 0.02549713\n",
      "Iteration 260, loss = 0.02539939\n",
      "Iteration 261, loss = 0.02533603\n",
      "Iteration 262, loss = 0.02525592\n",
      "Iteration 263, loss = 0.02513041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 264, loss = 0.02504651\n",
      "Iteration 265, loss = 0.02490334\n",
      "Iteration 266, loss = 0.02484639\n",
      "Iteration 267, loss = 0.02477712\n",
      "Iteration 268, loss = 0.02465826\n",
      "Iteration 269, loss = 0.02458047\n",
      "Iteration 270, loss = 0.02447431\n",
      "Iteration 271, loss = 0.02442121\n",
      "Iteration 272, loss = 0.02437501\n",
      "Iteration 273, loss = 0.02424217\n",
      "Iteration 274, loss = 0.02428654\n",
      "Iteration 275, loss = 0.02412672\n",
      "Iteration 276, loss = 0.02395977\n",
      "Iteration 277, loss = 0.02391220\n",
      "Iteration 278, loss = 0.02378615\n",
      "Iteration 279, loss = 0.02371016\n",
      "Iteration 280, loss = 0.02366278\n",
      "Iteration 281, loss = 0.02358331\n",
      "Iteration 282, loss = 0.02354632\n",
      "Iteration 283, loss = 0.02339882\n",
      "Iteration 284, loss = 0.02330447\n",
      "Iteration 285, loss = 0.02326734\n",
      "Iteration 286, loss = 0.02312539\n",
      "Iteration 287, loss = 0.02310106\n",
      "Iteration 288, loss = 0.02298716\n",
      "Iteration 289, loss = 0.02289707\n",
      "Iteration 290, loss = 0.02288905\n",
      "Iteration 291, loss = 0.02278956\n",
      "Iteration 292, loss = 0.02268894\n",
      "Iteration 293, loss = 0.02263788\n",
      "Iteration 294, loss = 0.02256807\n",
      "Iteration 295, loss = 0.02250743\n",
      "Iteration 296, loss = 0.02243412\n",
      "Iteration 297, loss = 0.02230684\n",
      "Iteration 298, loss = 0.02224639\n",
      "Iteration 299, loss = 0.02218264\n",
      "Iteration 300, loss = 0.02213631\n",
      "Iteration 301, loss = 0.02200233\n",
      "Iteration 302, loss = 0.02194724\n",
      "Iteration 303, loss = 0.02185049\n",
      "Iteration 304, loss = 0.02182226\n",
      "Iteration 305, loss = 0.02177531\n",
      "Iteration 306, loss = 0.02170312\n",
      "Iteration 307, loss = 0.02179148\n",
      "Iteration 308, loss = 0.02151730\n",
      "Iteration 309, loss = 0.02143434\n",
      "Iteration 310, loss = 0.02146955\n",
      "Iteration 311, loss = 0.02133993\n",
      "Iteration 312, loss = 0.02129225\n",
      "Iteration 313, loss = 0.02117288\n",
      "Iteration 314, loss = 0.02119755\n",
      "Iteration 315, loss = 0.02115215\n",
      "Iteration 316, loss = 0.02097588\n",
      "Iteration 317, loss = 0.02092467\n",
      "Iteration 318, loss = 0.02082611\n",
      "Iteration 319, loss = 0.02078394\n",
      "Iteration 320, loss = 0.02071503\n",
      "Iteration 321, loss = 0.02071269\n",
      "Iteration 322, loss = 0.02056914\n",
      "Iteration 323, loss = 0.02053383\n",
      "Iteration 324, loss = 0.02046349\n",
      "Iteration 325, loss = 0.02042875\n",
      "Iteration 326, loss = 0.02033478\n",
      "Iteration 327, loss = 0.02022284\n",
      "Iteration 328, loss = 0.02030927\n",
      "Iteration 329, loss = 0.02019799\n",
      "Iteration 330, loss = 0.02012973\n",
      "Iteration 331, loss = 0.02002604\n",
      "Iteration 332, loss = 0.01993641\n",
      "Iteration 333, loss = 0.01988243\n",
      "Iteration 334, loss = 0.01989098\n",
      "Iteration 335, loss = 0.01985347\n",
      "Iteration 336, loss = 0.01972810\n",
      "Iteration 337, loss = 0.01964502\n",
      "Iteration 338, loss = 0.01961561\n",
      "Iteration 339, loss = 0.01955737\n",
      "Iteration 340, loss = 0.01948051\n",
      "Iteration 341, loss = 0.01942851\n",
      "Iteration 342, loss = 0.01946823\n",
      "Iteration 343, loss = 0.01939272\n",
      "Iteration 344, loss = 0.01928866\n",
      "Iteration 345, loss = 0.01922114\n",
      "Iteration 346, loss = 0.01915107\n",
      "Iteration 347, loss = 0.01907048\n",
      "Iteration 348, loss = 0.01910158\n",
      "Iteration 349, loss = 0.01906580\n",
      "Iteration 350, loss = 0.01892442\n",
      "Iteration 351, loss = 0.01882833\n",
      "Iteration 352, loss = 0.01882738\n",
      "Iteration 353, loss = 0.01884809\n",
      "Iteration 354, loss = 0.01872347\n",
      "Iteration 355, loss = 0.01864477\n",
      "Iteration 356, loss = 0.01861032\n",
      "Iteration 357, loss = 0.01851699\n",
      "Iteration 358, loss = 0.01849241\n",
      "Iteration 359, loss = 0.01846464\n",
      "Iteration 360, loss = 0.01836875\n",
      "Iteration 361, loss = 0.01837565\n",
      "Iteration 362, loss = 0.01836633\n",
      "Iteration 363, loss = 0.01820401\n",
      "Iteration 364, loss = 0.01814445\n",
      "Iteration 365, loss = 0.01812241\n",
      "Iteration 366, loss = 0.01811801\n",
      "Iteration 367, loss = 0.01806991\n",
      "Iteration 368, loss = 0.01805762\n",
      "Iteration 369, loss = 0.01789990\n",
      "Iteration 370, loss = 0.01796196\n",
      "Iteration 371, loss = 0.01783012\n",
      "Iteration 372, loss = 0.01774323\n",
      "Iteration 373, loss = 0.01777032\n",
      "Iteration 374, loss = 0.01768246\n",
      "Iteration 375, loss = 0.01759645\n",
      "Iteration 376, loss = 0.01756301\n",
      "Iteration 377, loss = 0.01747932\n",
      "Iteration 378, loss = 0.01744271\n",
      "Iteration 379, loss = 0.01752042\n",
      "Iteration 380, loss = 0.01745145\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=1000, verbose=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural_credit = MLPClassifier(max_iter=1000, verbose=True)\n",
    "rede_neural_credit.fit(X_credit_train, y_credit_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = rede_neural_credit.predict(X_credit_test)\n",
    "previsoes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_credit_test, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(rede_neural_credit)\n",
    "cm.fit(X_credit_train, y_credit_train)\n",
    "cm.score(X_credit_test, y_credit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       0.97      0.97      0.97        64\n",
      "\n",
      "    accuracy                           0.99       500\n",
      "   macro avg       0.98      0.98      0.98       500\n",
      "weighted avg       0.99      0.99      0.99       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_credit_test,previsoes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('census.pkl', 'rb') as f:\n",
    "    X_census_train, y_census_train, X_census_test, y_census_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treino:  (27676, 108) (27676,)\n",
      "teste:  (4885, 108) (4885,)\n"
     ]
    }
   ],
   "source": [
    "print('treino: ',X_census_train.shape, y_census_train.shape)\n",
    "print('teste: ',X_census_test.shape, y_census_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
