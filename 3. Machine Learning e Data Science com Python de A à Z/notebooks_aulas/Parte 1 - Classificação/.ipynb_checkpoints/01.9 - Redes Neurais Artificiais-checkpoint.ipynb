{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceitos chave:\n",
    "(Supervisionado)\n",
    "\n",
    "- Feedforward\n",
    "    - Pesos aleatórios\n",
    "    - Função sigmoide\n",
    "    - Multicamada\n",
    "    - Camada oculta\n",
    "    - Função soma\n",
    "- Backpropagation\n",
    "    - Erros\n",
    "    - Delta saida\n",
    "    - Delta camada oculta\n",
    "    - Gradiente\n",
    "    - Taxa de aprendizagem\n",
    "    - Momento\n",
    "    - Ajuste dos pesos\n",
    "- Bias\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from yellowbrick.classifier import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Crédito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('credit.pkl', 'rb') as f:\n",
    "    X_credit_train, y_credit_train, X_credit_test, y_credit_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treino:  (1500, 3) (1500,)\n",
      "teste:  (500, 3) (500,)\n"
     ]
    }
   ],
   "source": [
    "print('treino: ',X_credit_train.shape, y_credit_train.shape)\n",
    "print('teste: ',X_credit_test.shape, y_credit_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.33767633\n",
      "Iteration 2, loss = 1.30887522\n",
      "Iteration 3, loss = 1.28064128\n",
      "Iteration 4, loss = 1.25401737\n",
      "Iteration 5, loss = 1.22821204\n",
      "Iteration 6, loss = 1.20362636\n",
      "Iteration 7, loss = 1.18093020\n",
      "Iteration 8, loss = 1.15870371\n",
      "Iteration 9, loss = 1.13770275\n",
      "Iteration 10, loss = 1.11795444\n",
      "Iteration 11, loss = 1.09908722\n",
      "Iteration 12, loss = 1.08117495\n",
      "Iteration 13, loss = 1.06407888\n",
      "Iteration 14, loss = 1.04801110\n",
      "Iteration 15, loss = 1.03232294\n",
      "Iteration 16, loss = 1.01767304\n",
      "Iteration 17, loss = 1.00359536\n",
      "Iteration 18, loss = 0.99022823\n",
      "Iteration 19, loss = 0.97780278\n",
      "Iteration 20, loss = 0.96548465\n",
      "Iteration 21, loss = 0.95397670\n",
      "Iteration 22, loss = 0.94290803\n",
      "Iteration 23, loss = 0.93243340\n",
      "Iteration 24, loss = 0.92206637\n",
      "Iteration 25, loss = 0.91261320\n",
      "Iteration 26, loss = 0.90329463\n",
      "Iteration 27, loss = 0.89444463\n",
      "Iteration 28, loss = 0.88591700\n",
      "Iteration 29, loss = 0.87780543\n",
      "Iteration 30, loss = 0.87007784\n",
      "Iteration 31, loss = 0.86247952\n",
      "Iteration 32, loss = 0.85521142\n",
      "Iteration 33, loss = 0.84833777\n",
      "Iteration 34, loss = 0.84159573\n",
      "Iteration 35, loss = 0.83497139\n",
      "Iteration 36, loss = 0.82869361\n",
      "Iteration 37, loss = 0.82261113\n",
      "Iteration 38, loss = 0.81668999\n",
      "Iteration 39, loss = 0.81091658\n",
      "Iteration 40, loss = 0.80537690\n",
      "Iteration 41, loss = 0.80003258\n",
      "Iteration 42, loss = 0.79475503\n",
      "Iteration 43, loss = 0.78957503\n",
      "Iteration 44, loss = 0.78472301\n",
      "Iteration 45, loss = 0.77976960\n",
      "Iteration 46, loss = 0.77509505\n",
      "Iteration 47, loss = 0.77048866\n",
      "Iteration 48, loss = 0.76594705\n",
      "Iteration 49, loss = 0.76151389\n",
      "Iteration 50, loss = 0.75715184\n",
      "Iteration 51, loss = 0.75297398\n",
      "Iteration 52, loss = 0.74884867\n",
      "Iteration 53, loss = 0.74483501\n",
      "Iteration 54, loss = 0.74087101\n",
      "Iteration 55, loss = 0.73698726\n",
      "Iteration 56, loss = 0.73316219\n",
      "Iteration 57, loss = 0.72944138\n",
      "Iteration 58, loss = 0.72582206\n",
      "Iteration 59, loss = 0.72224765\n",
      "Iteration 60, loss = 0.71872118\n",
      "Iteration 61, loss = 0.71529064\n",
      "Iteration 62, loss = 0.71188585\n",
      "Iteration 63, loss = 0.70861343\n",
      "Iteration 64, loss = 0.70533634\n",
      "Iteration 65, loss = 0.70209122\n",
      "Iteration 66, loss = 0.69898607\n",
      "Iteration 67, loss = 0.69585783\n",
      "Iteration 68, loss = 0.69282173\n",
      "Iteration 69, loss = 0.68984813\n",
      "Iteration 70, loss = 0.68687551\n",
      "Iteration 71, loss = 0.68400285\n",
      "Iteration 72, loss = 0.68119481\n",
      "Iteration 73, loss = 0.67842479\n",
      "Iteration 74, loss = 0.67568620\n",
      "Iteration 75, loss = 0.67294746\n",
      "Iteration 76, loss = 0.67030769\n",
      "Iteration 77, loss = 0.66769182\n",
      "Iteration 78, loss = 0.66507193\n",
      "Iteration 79, loss = 0.66248981\n",
      "Iteration 80, loss = 0.65994861\n",
      "Iteration 81, loss = 0.65744726\n",
      "Iteration 82, loss = 0.65495685\n",
      "Iteration 83, loss = 0.65250545\n",
      "Iteration 84, loss = 0.65004536\n",
      "Iteration 85, loss = 0.64763575\n",
      "Iteration 86, loss = 0.64524279\n",
      "Iteration 87, loss = 0.64286728\n",
      "Iteration 88, loss = 0.64050538\n",
      "Iteration 89, loss = 0.63815917\n",
      "Iteration 90, loss = 0.63588260\n",
      "Iteration 91, loss = 0.63360766\n",
      "Iteration 92, loss = 0.63134404\n",
      "Iteration 93, loss = 0.62911107\n",
      "Iteration 94, loss = 0.62693483\n",
      "Iteration 95, loss = 0.62471693\n",
      "Iteration 96, loss = 0.62253133\n",
      "Iteration 97, loss = 0.62038935\n",
      "Iteration 98, loss = 0.61830367\n",
      "Iteration 99, loss = 0.61616016\n",
      "Iteration 100, loss = 0.61408887\n",
      "Iteration 101, loss = 0.61200283\n",
      "Iteration 102, loss = 0.60994199\n",
      "Iteration 103, loss = 0.60791402\n",
      "Iteration 104, loss = 0.60586277\n",
      "Iteration 105, loss = 0.60386635\n",
      "Iteration 106, loss = 0.60184437\n",
      "Iteration 107, loss = 0.59988175\n",
      "Iteration 108, loss = 0.59787155\n",
      "Iteration 109, loss = 0.59592437\n",
      "Iteration 110, loss = 0.59395830\n",
      "Iteration 111, loss = 0.59203941\n",
      "Iteration 112, loss = 0.59006628\n",
      "Iteration 113, loss = 0.58814303\n",
      "Iteration 114, loss = 0.58621658\n",
      "Iteration 115, loss = 0.58430390\n",
      "Iteration 116, loss = 0.58239059\n",
      "Iteration 117, loss = 0.58049370\n",
      "Iteration 118, loss = 0.57859706\n",
      "Iteration 119, loss = 0.57669441\n",
      "Iteration 120, loss = 0.57479597\n",
      "Iteration 121, loss = 0.57291638\n",
      "Iteration 122, loss = 0.57104603\n",
      "Iteration 123, loss = 0.56917571\n",
      "Iteration 124, loss = 0.56730695\n",
      "Iteration 125, loss = 0.56543103\n",
      "Iteration 126, loss = 0.56359053\n",
      "Iteration 127, loss = 0.56173439\n",
      "Iteration 128, loss = 0.55986954\n",
      "Iteration 129, loss = 0.55802499\n",
      "Iteration 130, loss = 0.55618197\n",
      "Iteration 131, loss = 0.55435221\n",
      "Iteration 132, loss = 0.55253928\n",
      "Iteration 133, loss = 0.55072418\n",
      "Iteration 134, loss = 0.54890826\n",
      "Iteration 135, loss = 0.54710672\n",
      "Iteration 136, loss = 0.54528450\n",
      "Iteration 137, loss = 0.54349343\n",
      "Iteration 138, loss = 0.54168417\n",
      "Iteration 139, loss = 0.53989455\n",
      "Iteration 140, loss = 0.53810471\n",
      "Iteration 141, loss = 0.53631199\n",
      "Iteration 142, loss = 0.53453317\n",
      "Iteration 143, loss = 0.53273551\n",
      "Iteration 144, loss = 0.53094714\n",
      "Iteration 145, loss = 0.52914720\n",
      "Iteration 146, loss = 0.52735240\n",
      "Iteration 147, loss = 0.52555185\n",
      "Iteration 148, loss = 0.52375722\n",
      "Iteration 149, loss = 0.52193679\n",
      "Iteration 150, loss = 0.52012511\n",
      "Iteration 151, loss = 0.51831468\n",
      "Iteration 152, loss = 0.51649276\n",
      "Iteration 153, loss = 0.51468046\n",
      "Iteration 154, loss = 0.51285390\n",
      "Iteration 155, loss = 0.51103774\n",
      "Iteration 156, loss = 0.50919925\n",
      "Iteration 157, loss = 0.50735811\n",
      "Iteration 158, loss = 0.50551403\n",
      "Iteration 159, loss = 0.50368494\n",
      "Iteration 160, loss = 0.50180489\n",
      "Iteration 161, loss = 0.49995140\n",
      "Iteration 162, loss = 0.49809477\n",
      "Iteration 163, loss = 0.49621062\n",
      "Iteration 164, loss = 0.49433715\n",
      "Iteration 165, loss = 0.49245986\n",
      "Iteration 166, loss = 0.49055232\n",
      "Iteration 167, loss = 0.48866296\n",
      "Iteration 168, loss = 0.48674964\n",
      "Iteration 169, loss = 0.48485849\n",
      "Iteration 170, loss = 0.48293771\n",
      "Iteration 171, loss = 0.48100573\n",
      "Iteration 172, loss = 0.47908371\n",
      "Iteration 173, loss = 0.47712189\n",
      "Iteration 174, loss = 0.47517794\n",
      "Iteration 175, loss = 0.47319929\n",
      "Iteration 176, loss = 0.47125478\n",
      "Iteration 177, loss = 0.46924703\n",
      "Iteration 178, loss = 0.46726518\n",
      "Iteration 179, loss = 0.46529300\n",
      "Iteration 180, loss = 0.46330152\n",
      "Iteration 181, loss = 0.46130925\n",
      "Iteration 182, loss = 0.45932071\n",
      "Iteration 183, loss = 0.45730552\n",
      "Iteration 184, loss = 0.45530868\n",
      "Iteration 185, loss = 0.45330194\n",
      "Iteration 186, loss = 0.45130090\n",
      "Iteration 187, loss = 0.44927504\n",
      "Iteration 188, loss = 0.44727863\n",
      "Iteration 189, loss = 0.44529898\n",
      "Iteration 190, loss = 0.44331973\n",
      "Iteration 191, loss = 0.44132677\n",
      "Iteration 192, loss = 0.43933841\n",
      "Iteration 193, loss = 0.43733194\n",
      "Iteration 194, loss = 0.43535487\n",
      "Iteration 195, loss = 0.43333237\n",
      "Iteration 196, loss = 0.43132901\n",
      "Iteration 197, loss = 0.42934560\n",
      "Iteration 198, loss = 0.42733781\n",
      "Iteration 199, loss = 0.42533849\n",
      "Iteration 200, loss = 0.42336355\n",
      "Iteration 201, loss = 0.42135440\n",
      "Iteration 202, loss = 0.41934235\n",
      "Iteration 203, loss = 0.41732662\n",
      "Iteration 204, loss = 0.41531267\n",
      "Iteration 205, loss = 0.41330593\n",
      "Iteration 206, loss = 0.41125616\n",
      "Iteration 207, loss = 0.40921434\n",
      "Iteration 208, loss = 0.40720562\n",
      "Iteration 209, loss = 0.40519345\n",
      "Iteration 210, loss = 0.40319395\n",
      "Iteration 211, loss = 0.40117558\n",
      "Iteration 212, loss = 0.39922690\n",
      "Iteration 213, loss = 0.39726578\n",
      "Iteration 214, loss = 0.39530656\n",
      "Iteration 215, loss = 0.39336815\n",
      "Iteration 216, loss = 0.39142682\n",
      "Iteration 217, loss = 0.38948636\n",
      "Iteration 218, loss = 0.38755430\n",
      "Iteration 219, loss = 0.38566939\n",
      "Iteration 220, loss = 0.38371598\n",
      "Iteration 221, loss = 0.38182058\n",
      "Iteration 222, loss = 0.37993972\n",
      "Iteration 223, loss = 0.37806317\n",
      "Iteration 224, loss = 0.37617767\n",
      "Iteration 225, loss = 0.37437445\n",
      "Iteration 226, loss = 0.37247644\n",
      "Iteration 227, loss = 0.37064229\n",
      "Iteration 228, loss = 0.36883858\n",
      "Iteration 229, loss = 0.36699684\n",
      "Iteration 230, loss = 0.36519306\n",
      "Iteration 231, loss = 0.36339920\n",
      "Iteration 232, loss = 0.36159977\n",
      "Iteration 233, loss = 0.35982514\n",
      "Iteration 234, loss = 0.35803748\n",
      "Iteration 235, loss = 0.35626757\n",
      "Iteration 236, loss = 0.35454827\n",
      "Iteration 237, loss = 0.35278370\n",
      "Iteration 238, loss = 0.35104397\n",
      "Iteration 239, loss = 0.34932681\n",
      "Iteration 240, loss = 0.34760531\n",
      "Iteration 241, loss = 0.34588560\n",
      "Iteration 242, loss = 0.34421255\n",
      "Iteration 243, loss = 0.34254110\n",
      "Iteration 244, loss = 0.34082692\n",
      "Iteration 245, loss = 0.33915324\n",
      "Iteration 246, loss = 0.33750690\n",
      "Iteration 247, loss = 0.33585557\n",
      "Iteration 248, loss = 0.33422289\n",
      "Iteration 249, loss = 0.33260887\n",
      "Iteration 250, loss = 0.33098169\n",
      "Iteration 251, loss = 0.32939411\n",
      "Iteration 252, loss = 0.32778472\n",
      "Iteration 253, loss = 0.32621528\n",
      "Iteration 254, loss = 0.32467218\n",
      "Iteration 255, loss = 0.32308056\n",
      "Iteration 256, loss = 0.32151980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 257, loss = 0.31997839\n",
      "Iteration 258, loss = 0.31842698\n",
      "Iteration 259, loss = 0.31689714\n",
      "Iteration 260, loss = 0.31538605\n",
      "Iteration 261, loss = 0.31388439\n",
      "Iteration 262, loss = 0.31237104\n",
      "Iteration 263, loss = 0.31093937\n",
      "Iteration 264, loss = 0.30942774\n",
      "Iteration 265, loss = 0.30791658\n",
      "Iteration 266, loss = 0.30645125\n",
      "Iteration 267, loss = 0.30504157\n",
      "Iteration 268, loss = 0.30357480\n",
      "Iteration 269, loss = 0.30213840\n",
      "Iteration 270, loss = 0.30069293\n",
      "Iteration 271, loss = 0.29926902\n",
      "Iteration 272, loss = 0.29786289\n",
      "Iteration 273, loss = 0.29644529\n",
      "Iteration 274, loss = 0.29508678\n",
      "Iteration 275, loss = 0.29368419\n",
      "Iteration 276, loss = 0.29233421\n",
      "Iteration 277, loss = 0.29092698\n",
      "Iteration 278, loss = 0.28959188\n",
      "Iteration 279, loss = 0.28821652\n",
      "Iteration 280, loss = 0.28687040\n",
      "Iteration 281, loss = 0.28554641\n",
      "Iteration 282, loss = 0.28420477\n",
      "Iteration 283, loss = 0.28290619\n",
      "Iteration 284, loss = 0.28160415\n",
      "Iteration 285, loss = 0.28030624\n",
      "Iteration 286, loss = 0.27901305\n",
      "Iteration 287, loss = 0.27771715\n",
      "Iteration 288, loss = 0.27644425\n",
      "Iteration 289, loss = 0.27522074\n",
      "Iteration 290, loss = 0.27393591\n",
      "Iteration 291, loss = 0.27269915\n",
      "Iteration 292, loss = 0.27145514\n",
      "Iteration 293, loss = 0.27022456\n",
      "Iteration 294, loss = 0.26899998\n",
      "Iteration 295, loss = 0.26778972\n",
      "Iteration 296, loss = 0.26659483\n",
      "Iteration 297, loss = 0.26536761\n",
      "Iteration 298, loss = 0.26418664\n",
      "Iteration 299, loss = 0.26298456\n",
      "Iteration 300, loss = 0.26186550\n",
      "Iteration 301, loss = 0.26066939\n",
      "Iteration 302, loss = 0.25951207\n",
      "Iteration 303, loss = 0.25833306\n",
      "Iteration 304, loss = 0.25719034\n",
      "Iteration 305, loss = 0.25603807\n",
      "Iteration 306, loss = 0.25489852\n",
      "Iteration 307, loss = 0.25379924\n",
      "Iteration 308, loss = 0.25264217\n",
      "Iteration 309, loss = 0.25153039\n",
      "Iteration 310, loss = 0.25037169\n",
      "Iteration 311, loss = 0.24925272\n",
      "Iteration 312, loss = 0.24813918\n",
      "Iteration 313, loss = 0.24705992\n",
      "Iteration 314, loss = 0.24594268\n",
      "Iteration 315, loss = 0.24482620\n",
      "Iteration 316, loss = 0.24375064\n",
      "Iteration 317, loss = 0.24267060\n",
      "Iteration 318, loss = 0.24159806\n",
      "Iteration 319, loss = 0.24053633\n",
      "Iteration 320, loss = 0.23946808\n",
      "Iteration 321, loss = 0.23840108\n",
      "Iteration 322, loss = 0.23735729\n",
      "Iteration 323, loss = 0.23632576\n",
      "Iteration 324, loss = 0.23524577\n",
      "Iteration 325, loss = 0.23419223\n",
      "Iteration 326, loss = 0.23316129\n",
      "Iteration 327, loss = 0.23217047\n",
      "Iteration 328, loss = 0.23113980\n",
      "Iteration 329, loss = 0.23012397\n",
      "Iteration 330, loss = 0.22910782\n",
      "Iteration 331, loss = 0.22810029\n",
      "Iteration 332, loss = 0.22710188\n",
      "Iteration 333, loss = 0.22608572\n",
      "Iteration 334, loss = 0.22513624\n",
      "Iteration 335, loss = 0.22411844\n",
      "Iteration 336, loss = 0.22311898\n",
      "Iteration 337, loss = 0.22215305\n",
      "Iteration 338, loss = 0.22114503\n",
      "Iteration 339, loss = 0.22020120\n",
      "Iteration 340, loss = 0.21922579\n",
      "Iteration 341, loss = 0.21824327\n",
      "Iteration 342, loss = 0.21727816\n",
      "Iteration 343, loss = 0.21634280\n",
      "Iteration 344, loss = 0.21540279\n",
      "Iteration 345, loss = 0.21445843\n",
      "Iteration 346, loss = 0.21351333\n",
      "Iteration 347, loss = 0.21258672\n",
      "Iteration 348, loss = 0.21164074\n",
      "Iteration 349, loss = 0.21073063\n",
      "Iteration 350, loss = 0.20981545\n",
      "Iteration 351, loss = 0.20890655\n",
      "Iteration 352, loss = 0.20800596\n",
      "Iteration 353, loss = 0.20711910\n",
      "Iteration 354, loss = 0.20620607\n",
      "Iteration 355, loss = 0.20534462\n",
      "Iteration 356, loss = 0.20445569\n",
      "Iteration 357, loss = 0.20355712\n",
      "Iteration 358, loss = 0.20269448\n",
      "Iteration 359, loss = 0.20183008\n",
      "Iteration 360, loss = 0.20095567\n",
      "Iteration 361, loss = 0.20007146\n",
      "Iteration 362, loss = 0.19922197\n",
      "Iteration 363, loss = 0.19835938\n",
      "Iteration 364, loss = 0.19750646\n",
      "Iteration 365, loss = 0.19664917\n",
      "Iteration 366, loss = 0.19581373\n",
      "Iteration 367, loss = 0.19494683\n",
      "Iteration 368, loss = 0.19410383\n",
      "Iteration 369, loss = 0.19326858\n",
      "Iteration 370, loss = 0.19242832\n",
      "Iteration 371, loss = 0.19163911\n",
      "Iteration 372, loss = 0.19080830\n",
      "Iteration 373, loss = 0.18998828\n",
      "Iteration 374, loss = 0.18918296\n",
      "Iteration 375, loss = 0.18835180\n",
      "Iteration 376, loss = 0.18756345\n",
      "Iteration 377, loss = 0.18676912\n",
      "Iteration 378, loss = 0.18596225\n",
      "Iteration 379, loss = 0.18522451\n",
      "Iteration 380, loss = 0.18443328\n",
      "Iteration 381, loss = 0.18364336\n",
      "Iteration 382, loss = 0.18286340\n",
      "Iteration 383, loss = 0.18210301\n",
      "Iteration 384, loss = 0.18133030\n",
      "Iteration 385, loss = 0.18056160\n",
      "Iteration 386, loss = 0.17981088\n",
      "Iteration 387, loss = 0.17906016\n",
      "Iteration 388, loss = 0.17833798\n",
      "Iteration 389, loss = 0.17761447\n",
      "Iteration 390, loss = 0.17681923\n",
      "Iteration 391, loss = 0.17609454\n",
      "Iteration 392, loss = 0.17540579\n",
      "Iteration 393, loss = 0.17464180\n",
      "Iteration 394, loss = 0.17391243\n",
      "Iteration 395, loss = 0.17321134\n",
      "Iteration 396, loss = 0.17243452\n",
      "Iteration 397, loss = 0.17173387\n",
      "Iteration 398, loss = 0.17102415\n",
      "Iteration 399, loss = 0.17034996\n",
      "Iteration 400, loss = 0.16969144\n",
      "Iteration 401, loss = 0.16890067\n",
      "Iteration 402, loss = 0.16818639\n",
      "Iteration 403, loss = 0.16751013\n",
      "Iteration 404, loss = 0.16680750\n",
      "Iteration 405, loss = 0.16614227\n",
      "Iteration 406, loss = 0.16542432\n",
      "Iteration 407, loss = 0.16476641\n",
      "Iteration 408, loss = 0.16406647\n",
      "Iteration 409, loss = 0.16339249\n",
      "Iteration 410, loss = 0.16276595\n",
      "Iteration 411, loss = 0.16204469\n",
      "Iteration 412, loss = 0.16140641\n",
      "Iteration 413, loss = 0.16073083\n",
      "Iteration 414, loss = 0.16007701\n",
      "Iteration 415, loss = 0.15941377\n",
      "Iteration 416, loss = 0.15879544\n",
      "Iteration 417, loss = 0.15809222\n",
      "Iteration 418, loss = 0.15748055\n",
      "Iteration 419, loss = 0.15681786\n",
      "Iteration 420, loss = 0.15618089\n",
      "Iteration 421, loss = 0.15555971\n",
      "Iteration 422, loss = 0.15494338\n",
      "Iteration 423, loss = 0.15429549\n",
      "Iteration 424, loss = 0.15368433\n",
      "Iteration 425, loss = 0.15306362\n",
      "Iteration 426, loss = 0.15244231\n",
      "Iteration 427, loss = 0.15182292\n",
      "Iteration 428, loss = 0.15121626\n",
      "Iteration 429, loss = 0.15060756\n",
      "Iteration 430, loss = 0.15001672\n",
      "Iteration 431, loss = 0.14941287\n",
      "Iteration 432, loss = 0.14879188\n",
      "Iteration 433, loss = 0.14822444\n",
      "Iteration 434, loss = 0.14760911\n",
      "Iteration 435, loss = 0.14699692\n",
      "Iteration 436, loss = 0.14640115\n",
      "Iteration 437, loss = 0.14585876\n",
      "Iteration 438, loss = 0.14523071\n",
      "Iteration 439, loss = 0.14465082\n",
      "Iteration 440, loss = 0.14406558\n",
      "Iteration 441, loss = 0.14352641\n",
      "Iteration 442, loss = 0.14294641\n",
      "Iteration 443, loss = 0.14236397\n",
      "Iteration 444, loss = 0.14178175\n",
      "Iteration 445, loss = 0.14121026\n",
      "Iteration 446, loss = 0.14066239\n",
      "Iteration 447, loss = 0.14010200\n",
      "Iteration 448, loss = 0.13953389\n",
      "Iteration 449, loss = 0.13899998\n",
      "Iteration 450, loss = 0.13844778\n",
      "Iteration 451, loss = 0.13792719\n",
      "Iteration 452, loss = 0.13733227\n",
      "Iteration 453, loss = 0.13682223\n",
      "Iteration 454, loss = 0.13624333\n",
      "Iteration 455, loss = 0.13570058\n",
      "Iteration 456, loss = 0.13519915\n",
      "Iteration 457, loss = 0.13464596\n",
      "Iteration 458, loss = 0.13414379\n",
      "Iteration 459, loss = 0.13360533\n",
      "Iteration 460, loss = 0.13307995\n",
      "Iteration 461, loss = 0.13252792\n",
      "Iteration 462, loss = 0.13204687\n",
      "Iteration 463, loss = 0.13150538\n",
      "Iteration 464, loss = 0.13097864\n",
      "Iteration 465, loss = 0.13051443\n",
      "Iteration 466, loss = 0.12998637\n",
      "Iteration 467, loss = 0.12948614\n",
      "Iteration 468, loss = 0.12899048\n",
      "Iteration 469, loss = 0.12849786\n",
      "Iteration 470, loss = 0.12802826\n",
      "Iteration 471, loss = 0.12750000\n",
      "Iteration 472, loss = 0.12702850\n",
      "Iteration 473, loss = 0.12652481\n",
      "Iteration 474, loss = 0.12605382\n",
      "Iteration 475, loss = 0.12554707\n",
      "Iteration 476, loss = 0.12507564\n",
      "Iteration 477, loss = 0.12456115\n",
      "Iteration 478, loss = 0.12411298\n",
      "Iteration 479, loss = 0.12367729\n",
      "Iteration 480, loss = 0.12317965\n",
      "Iteration 481, loss = 0.12271705\n",
      "Iteration 482, loss = 0.12222561\n",
      "Iteration 483, loss = 0.12179078\n",
      "Iteration 484, loss = 0.12130501\n",
      "Iteration 485, loss = 0.12087491\n",
      "Iteration 486, loss = 0.12042302\n",
      "Iteration 487, loss = 0.11996051\n",
      "Iteration 488, loss = 0.11948417\n",
      "Iteration 489, loss = 0.11903880\n",
      "Iteration 490, loss = 0.11861238\n",
      "Iteration 491, loss = 0.11815669\n",
      "Iteration 492, loss = 0.11774174\n",
      "Iteration 493, loss = 0.11726270\n",
      "Iteration 494, loss = 0.11683027\n",
      "Iteration 495, loss = 0.11639117\n",
      "Iteration 496, loss = 0.11596342\n",
      "Iteration 497, loss = 0.11551371\n",
      "Iteration 498, loss = 0.11505079\n",
      "Iteration 499, loss = 0.11463495\n",
      "Iteration 500, loss = 0.11420181\n",
      "Iteration 501, loss = 0.11382699\n",
      "Iteration 502, loss = 0.11340647\n",
      "Iteration 503, loss = 0.11295478\n",
      "Iteration 504, loss = 0.11252044\n",
      "Iteration 505, loss = 0.11209720\n",
      "Iteration 506, loss = 0.11168077\n",
      "Iteration 507, loss = 0.11132358\n",
      "Iteration 508, loss = 0.11082820\n",
      "Iteration 509, loss = 0.11047102\n",
      "Iteration 510, loss = 0.11007145\n",
      "Iteration 511, loss = 0.10963538\n",
      "Iteration 512, loss = 0.10926386\n",
      "Iteration 513, loss = 0.10883977\n",
      "Iteration 514, loss = 0.10841718\n",
      "Iteration 515, loss = 0.10804977\n",
      "Iteration 516, loss = 0.10761701\n",
      "Iteration 517, loss = 0.10722211\n",
      "Iteration 518, loss = 0.10685180\n",
      "Iteration 519, loss = 0.10643486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 520, loss = 0.10605553\n",
      "Iteration 521, loss = 0.10567921\n",
      "Iteration 522, loss = 0.10530514\n",
      "Iteration 523, loss = 0.10491079\n",
      "Iteration 524, loss = 0.10452316\n",
      "Iteration 525, loss = 0.10416152\n",
      "Iteration 526, loss = 0.10375029\n",
      "Iteration 527, loss = 0.10335393\n",
      "Iteration 528, loss = 0.10298104\n",
      "Iteration 529, loss = 0.10264916\n",
      "Iteration 530, loss = 0.10225698\n",
      "Iteration 531, loss = 0.10187193\n",
      "Iteration 532, loss = 0.10154985\n",
      "Iteration 533, loss = 0.10112718\n",
      "Iteration 534, loss = 0.10078200\n",
      "Iteration 535, loss = 0.10040921\n",
      "Iteration 536, loss = 0.10005420\n",
      "Iteration 537, loss = 0.09968625\n",
      "Iteration 538, loss = 0.09934514\n",
      "Iteration 539, loss = 0.09893934\n",
      "Iteration 540, loss = 0.09860063\n",
      "Iteration 541, loss = 0.09826856\n",
      "Iteration 542, loss = 0.09788008\n",
      "Iteration 543, loss = 0.09760521\n",
      "Iteration 544, loss = 0.09717700\n",
      "Iteration 545, loss = 0.09683413\n",
      "Iteration 546, loss = 0.09647285\n",
      "Iteration 547, loss = 0.09620075\n",
      "Iteration 548, loss = 0.09578491\n",
      "Iteration 549, loss = 0.09545912\n",
      "Iteration 550, loss = 0.09510648\n",
      "Iteration 551, loss = 0.09478009\n",
      "Iteration 552, loss = 0.09442505\n",
      "Iteration 553, loss = 0.09411140\n",
      "Iteration 554, loss = 0.09375643\n",
      "Iteration 555, loss = 0.09346745\n",
      "Iteration 556, loss = 0.09308875\n",
      "Iteration 557, loss = 0.09278344\n",
      "Iteration 558, loss = 0.09242338\n",
      "Iteration 559, loss = 0.09208940\n",
      "Iteration 560, loss = 0.09177541\n",
      "Iteration 561, loss = 0.09144692\n",
      "Iteration 562, loss = 0.09115057\n",
      "Iteration 563, loss = 0.09081568\n",
      "Iteration 564, loss = 0.09048218\n",
      "Iteration 565, loss = 0.09017884\n",
      "Iteration 566, loss = 0.08989175\n",
      "Iteration 567, loss = 0.08952820\n",
      "Iteration 568, loss = 0.08930443\n",
      "Iteration 569, loss = 0.08893946\n",
      "Iteration 570, loss = 0.08860018\n",
      "Iteration 571, loss = 0.08834910\n",
      "Iteration 572, loss = 0.08803632\n",
      "Iteration 573, loss = 0.08766564\n",
      "Iteration 574, loss = 0.08736108\n",
      "Iteration 575, loss = 0.08706751\n",
      "Iteration 576, loss = 0.08674524\n",
      "Iteration 577, loss = 0.08648480\n",
      "Iteration 578, loss = 0.08613297\n",
      "Iteration 579, loss = 0.08584622\n",
      "Iteration 580, loss = 0.08552690\n",
      "Iteration 581, loss = 0.08524905\n",
      "Iteration 582, loss = 0.08494424\n",
      "Iteration 583, loss = 0.08465021\n",
      "Iteration 584, loss = 0.08434645\n",
      "Iteration 585, loss = 0.08405902\n",
      "Iteration 586, loss = 0.08376716\n",
      "Iteration 587, loss = 0.08347500\n",
      "Iteration 588, loss = 0.08318548\n",
      "Iteration 589, loss = 0.08290996\n",
      "Iteration 590, loss = 0.08260543\n",
      "Iteration 591, loss = 0.08232821\n",
      "Iteration 592, loss = 0.08205962\n",
      "Iteration 593, loss = 0.08178099\n",
      "Iteration 594, loss = 0.08148740\n",
      "Iteration 595, loss = 0.08121385\n",
      "Iteration 596, loss = 0.08092190\n",
      "Iteration 597, loss = 0.08064746\n",
      "Iteration 598, loss = 0.08036624\n",
      "Iteration 599, loss = 0.08009164\n",
      "Iteration 600, loss = 0.07982494\n",
      "Iteration 601, loss = 0.07959624\n",
      "Iteration 602, loss = 0.07926921\n",
      "Iteration 603, loss = 0.07900021\n",
      "Iteration 604, loss = 0.07874303\n",
      "Iteration 605, loss = 0.07846266\n",
      "Iteration 606, loss = 0.07817003\n",
      "Iteration 607, loss = 0.07793497\n",
      "Iteration 608, loss = 0.07767145\n",
      "Iteration 609, loss = 0.07740172\n",
      "Iteration 610, loss = 0.07714541\n",
      "Iteration 611, loss = 0.07689555\n",
      "Iteration 612, loss = 0.07663236\n",
      "Iteration 613, loss = 0.07633870\n",
      "Iteration 614, loss = 0.07610589\n",
      "Iteration 615, loss = 0.07582174\n",
      "Iteration 616, loss = 0.07558650\n",
      "Iteration 617, loss = 0.07531384\n",
      "Iteration 618, loss = 0.07508679\n",
      "Iteration 619, loss = 0.07481603\n",
      "Iteration 620, loss = 0.07457841\n",
      "Iteration 621, loss = 0.07429558\n",
      "Iteration 622, loss = 0.07404901\n",
      "Iteration 623, loss = 0.07379966\n",
      "Iteration 624, loss = 0.07354328\n",
      "Iteration 625, loss = 0.07330993\n",
      "Iteration 626, loss = 0.07308662\n",
      "Iteration 627, loss = 0.07285932\n",
      "Iteration 628, loss = 0.07257893\n",
      "Iteration 629, loss = 0.07232206\n",
      "Iteration 630, loss = 0.07213241\n",
      "Iteration 631, loss = 0.07185310\n",
      "Iteration 632, loss = 0.07161059\n",
      "Iteration 633, loss = 0.07137854\n",
      "Iteration 634, loss = 0.07111400\n",
      "Iteration 635, loss = 0.07090381\n",
      "Iteration 636, loss = 0.07071260\n",
      "Iteration 637, loss = 0.07042240\n",
      "Iteration 638, loss = 0.07017511\n",
      "Iteration 639, loss = 0.06997677\n",
      "Iteration 640, loss = 0.06971522\n",
      "Iteration 641, loss = 0.06947803\n",
      "Iteration 642, loss = 0.06925681\n",
      "Iteration 643, loss = 0.06902589\n",
      "Iteration 644, loss = 0.06881367\n",
      "Iteration 645, loss = 0.06856474\n",
      "Iteration 646, loss = 0.06836236\n",
      "Iteration 647, loss = 0.06809882\n",
      "Iteration 648, loss = 0.06789731\n",
      "Iteration 649, loss = 0.06772672\n",
      "Iteration 650, loss = 0.06742544\n",
      "Iteration 651, loss = 0.06718655\n",
      "Iteration 652, loss = 0.06705827\n",
      "Iteration 653, loss = 0.06679420\n",
      "Iteration 654, loss = 0.06657904\n",
      "Iteration 655, loss = 0.06636385\n",
      "Iteration 656, loss = 0.06612690\n",
      "Iteration 657, loss = 0.06609041\n",
      "Iteration 658, loss = 0.06570380\n",
      "Iteration 659, loss = 0.06548862\n",
      "Iteration 660, loss = 0.06524510\n",
      "Iteration 661, loss = 0.06507971\n",
      "Iteration 662, loss = 0.06480446\n",
      "Iteration 663, loss = 0.06461959\n",
      "Iteration 664, loss = 0.06439993\n",
      "Iteration 665, loss = 0.06420479\n",
      "Iteration 666, loss = 0.06398767\n",
      "Iteration 667, loss = 0.06379424\n",
      "Iteration 668, loss = 0.06355006\n",
      "Iteration 669, loss = 0.06336101\n",
      "Iteration 670, loss = 0.06313584\n",
      "Iteration 671, loss = 0.06297359\n",
      "Iteration 672, loss = 0.06278071\n",
      "Iteration 673, loss = 0.06257904\n",
      "Iteration 674, loss = 0.06232836\n",
      "Iteration 675, loss = 0.06213945\n",
      "Iteration 676, loss = 0.06193299\n",
      "Iteration 677, loss = 0.06173771\n",
      "Iteration 678, loss = 0.06150890\n",
      "Iteration 679, loss = 0.06135061\n",
      "Iteration 680, loss = 0.06116315\n",
      "Iteration 681, loss = 0.06094574\n",
      "Iteration 682, loss = 0.06082511\n",
      "Iteration 683, loss = 0.06058291\n",
      "Iteration 684, loss = 0.06035728\n",
      "Iteration 685, loss = 0.06015967\n",
      "Iteration 686, loss = 0.05993775\n",
      "Iteration 687, loss = 0.05974384\n",
      "Iteration 688, loss = 0.05955793\n",
      "Iteration 689, loss = 0.05937894\n",
      "Iteration 690, loss = 0.05919857\n",
      "Iteration 691, loss = 0.05902541\n",
      "Iteration 692, loss = 0.05882835\n",
      "Iteration 693, loss = 0.05860711\n",
      "Iteration 694, loss = 0.05844686\n",
      "Iteration 695, loss = 0.05830923\n",
      "Iteration 696, loss = 0.05804695\n",
      "Iteration 697, loss = 0.05785563\n",
      "Iteration 698, loss = 0.05766631\n",
      "Iteration 699, loss = 0.05748256\n",
      "Iteration 700, loss = 0.05735025\n",
      "Iteration 701, loss = 0.05713829\n",
      "Iteration 702, loss = 0.05694972\n",
      "Iteration 703, loss = 0.05680149\n",
      "Iteration 704, loss = 0.05657493\n",
      "Iteration 705, loss = 0.05640744\n",
      "Iteration 706, loss = 0.05622666\n",
      "Iteration 707, loss = 0.05608970\n",
      "Iteration 708, loss = 0.05585990\n",
      "Iteration 709, loss = 0.05569095\n",
      "Iteration 710, loss = 0.05556150\n",
      "Iteration 711, loss = 0.05532671\n",
      "Iteration 712, loss = 0.05514878\n",
      "Iteration 713, loss = 0.05497150\n",
      "Iteration 714, loss = 0.05481933\n",
      "Iteration 715, loss = 0.05462294\n",
      "Iteration 716, loss = 0.05449900\n",
      "Iteration 717, loss = 0.05428471\n",
      "Iteration 718, loss = 0.05411414\n",
      "Iteration 719, loss = 0.05397554\n",
      "Iteration 720, loss = 0.05380911\n",
      "Iteration 721, loss = 0.05361302\n",
      "Iteration 722, loss = 0.05342911\n",
      "Iteration 723, loss = 0.05325468\n",
      "Iteration 724, loss = 0.05310405\n",
      "Iteration 725, loss = 0.05295385\n",
      "Iteration 726, loss = 0.05280838\n",
      "Iteration 727, loss = 0.05263439\n",
      "Iteration 728, loss = 0.05244333\n",
      "Iteration 729, loss = 0.05227158\n",
      "Iteration 730, loss = 0.05209200\n",
      "Iteration 731, loss = 0.05195143\n",
      "Iteration 732, loss = 0.05182886\n",
      "Iteration 733, loss = 0.05164201\n",
      "Iteration 734, loss = 0.05147175\n",
      "Iteration 735, loss = 0.05130282\n",
      "Iteration 736, loss = 0.05113723\n",
      "Iteration 737, loss = 0.05101635\n",
      "Iteration 738, loss = 0.05081819\n",
      "Iteration 739, loss = 0.05070132\n",
      "Iteration 740, loss = 0.05049456\n",
      "Iteration 741, loss = 0.05032976\n",
      "Iteration 742, loss = 0.05019987\n",
      "Iteration 743, loss = 0.05002332\n",
      "Iteration 744, loss = 0.04987203\n",
      "Iteration 745, loss = 0.04974144\n",
      "Iteration 746, loss = 0.04955484\n",
      "Iteration 747, loss = 0.04939898\n",
      "Iteration 748, loss = 0.04927064\n",
      "Iteration 749, loss = 0.04909715\n",
      "Iteration 750, loss = 0.04893710\n",
      "Iteration 751, loss = 0.04880030\n",
      "Iteration 752, loss = 0.04864769\n",
      "Iteration 753, loss = 0.04851051\n",
      "Iteration 754, loss = 0.04835877\n",
      "Iteration 755, loss = 0.04823725\n",
      "Iteration 756, loss = 0.04805632\n",
      "Iteration 757, loss = 0.04789941\n",
      "Iteration 758, loss = 0.04773923\n",
      "Iteration 759, loss = 0.04759888\n",
      "Iteration 760, loss = 0.04746399\n",
      "Iteration 761, loss = 0.04730246\n",
      "Iteration 762, loss = 0.04713383\n",
      "Iteration 763, loss = 0.04701378\n",
      "Iteration 764, loss = 0.04687200\n",
      "Iteration 765, loss = 0.04672690\n",
      "Iteration 766, loss = 0.04659587\n",
      "Iteration 767, loss = 0.04643140\n",
      "Iteration 768, loss = 0.04635634\n",
      "Iteration 769, loss = 0.04614701\n",
      "Iteration 770, loss = 0.04601041\n",
      "Iteration 771, loss = 0.04590268\n",
      "Iteration 772, loss = 0.04571744\n",
      "Iteration 773, loss = 0.04561148\n",
      "Iteration 774, loss = 0.04543408\n",
      "Iteration 775, loss = 0.04530300\n",
      "Iteration 776, loss = 0.04516146\n",
      "Iteration 777, loss = 0.04501632\n",
      "Iteration 778, loss = 0.04489165\n",
      "Iteration 779, loss = 0.04478390\n",
      "Iteration 780, loss = 0.04460428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 781, loss = 0.04447699\n",
      "Iteration 782, loss = 0.04433825\n",
      "Iteration 783, loss = 0.04415799\n",
      "Iteration 784, loss = 0.04406822\n",
      "Iteration 785, loss = 0.04394411\n",
      "Iteration 786, loss = 0.04387386\n",
      "Iteration 787, loss = 0.04367834\n",
      "Iteration 788, loss = 0.04351617\n",
      "Iteration 789, loss = 0.04339890\n",
      "Iteration 790, loss = 0.04329174\n",
      "Iteration 791, loss = 0.04315295\n",
      "Iteration 792, loss = 0.04303259\n",
      "Iteration 793, loss = 0.04288238\n",
      "Iteration 794, loss = 0.04273159\n",
      "Iteration 795, loss = 0.04261066\n",
      "Iteration 796, loss = 0.04248331\n",
      "Iteration 797, loss = 0.04237103\n",
      "Iteration 798, loss = 0.04222012\n",
      "Iteration 799, loss = 0.04211531\n",
      "Iteration 800, loss = 0.04198842\n",
      "Iteration 801, loss = 0.04183891\n",
      "Iteration 802, loss = 0.04170049\n",
      "Iteration 803, loss = 0.04159221\n",
      "Iteration 804, loss = 0.04148315\n",
      "Iteration 805, loss = 0.04135262\n",
      "Iteration 806, loss = 0.04122854\n",
      "Iteration 807, loss = 0.04107690\n",
      "Iteration 808, loss = 0.04098657\n",
      "Iteration 809, loss = 0.04085387\n",
      "Iteration 810, loss = 0.04072715\n",
      "Iteration 811, loss = 0.04067035\n",
      "Iteration 812, loss = 0.04050599\n",
      "Iteration 813, loss = 0.04037451\n",
      "Iteration 814, loss = 0.04024010\n",
      "Iteration 815, loss = 0.04016612\n",
      "Iteration 816, loss = 0.03998923\n",
      "Iteration 817, loss = 0.03993107\n",
      "Iteration 818, loss = 0.03978568\n",
      "Iteration 819, loss = 0.03971105\n",
      "Iteration 820, loss = 0.03952178\n",
      "Iteration 821, loss = 0.03944292\n",
      "Iteration 822, loss = 0.03927008\n",
      "Iteration 823, loss = 0.03915722\n",
      "Iteration 824, loss = 0.03904645\n",
      "Iteration 825, loss = 0.03897919\n",
      "Iteration 826, loss = 0.03880338\n",
      "Iteration 827, loss = 0.03869454\n",
      "Iteration 828, loss = 0.03859417\n",
      "Iteration 829, loss = 0.03853516\n",
      "Iteration 830, loss = 0.03835252\n",
      "Iteration 831, loss = 0.03825975\n",
      "Iteration 832, loss = 0.03812365\n",
      "Iteration 833, loss = 0.03802872\n",
      "Iteration 834, loss = 0.03791476\n",
      "Iteration 835, loss = 0.03778707\n",
      "Iteration 836, loss = 0.03768520\n",
      "Iteration 837, loss = 0.03757907\n",
      "Iteration 838, loss = 0.03745708\n",
      "Iteration 839, loss = 0.03734748\n",
      "Iteration 840, loss = 0.03723627\n",
      "Iteration 841, loss = 0.03713365\n",
      "Iteration 842, loss = 0.03701459\n",
      "Iteration 843, loss = 0.03690069\n",
      "Iteration 844, loss = 0.03682941\n",
      "Iteration 845, loss = 0.03668773\n",
      "Iteration 846, loss = 0.03658392\n",
      "Iteration 847, loss = 0.03656386\n",
      "Iteration 848, loss = 0.03636728\n",
      "Iteration 849, loss = 0.03625932\n",
      "Iteration 850, loss = 0.03619517\n",
      "Iteration 851, loss = 0.03607330\n",
      "Iteration 852, loss = 0.03597809\n",
      "Iteration 853, loss = 0.03584460\n",
      "Iteration 854, loss = 0.03574397\n",
      "Iteration 855, loss = 0.03562622\n",
      "Iteration 856, loss = 0.03555287\n",
      "Iteration 857, loss = 0.03544894\n",
      "Iteration 858, loss = 0.03535017\n",
      "Iteration 859, loss = 0.03520466\n",
      "Iteration 860, loss = 0.03510296\n",
      "Iteration 861, loss = 0.03501202\n",
      "Iteration 862, loss = 0.03489946\n",
      "Iteration 863, loss = 0.03481400\n",
      "Iteration 864, loss = 0.03468370\n",
      "Iteration 865, loss = 0.03459401\n",
      "Iteration 866, loss = 0.03448921\n",
      "Iteration 867, loss = 0.03441334\n",
      "Iteration 868, loss = 0.03429685\n",
      "Iteration 869, loss = 0.03419630\n",
      "Iteration 870, loss = 0.03408234\n",
      "Iteration 871, loss = 0.03397147\n",
      "Iteration 872, loss = 0.03391514\n",
      "Iteration 873, loss = 0.03381490\n",
      "Iteration 874, loss = 0.03370252\n",
      "Iteration 875, loss = 0.03358518\n",
      "Iteration 876, loss = 0.03356804\n",
      "Iteration 877, loss = 0.03342880\n",
      "Iteration 878, loss = 0.03345905\n",
      "Iteration 879, loss = 0.03325169\n",
      "Iteration 880, loss = 0.03314318\n",
      "Iteration 881, loss = 0.03302653\n",
      "Iteration 882, loss = 0.03292202\n",
      "Iteration 883, loss = 0.03285082\n",
      "Iteration 884, loss = 0.03273546\n",
      "Iteration 885, loss = 0.03266469\n",
      "Iteration 886, loss = 0.03254929\n",
      "Iteration 887, loss = 0.03250163\n",
      "Iteration 888, loss = 0.03240574\n",
      "Iteration 889, loss = 0.03237750\n",
      "Iteration 890, loss = 0.03222687\n",
      "Iteration 891, loss = 0.03208377\n",
      "Iteration 892, loss = 0.03198260\n",
      "Iteration 893, loss = 0.03189792\n",
      "Iteration 894, loss = 0.03181449\n",
      "Iteration 895, loss = 0.03176173\n",
      "Iteration 896, loss = 0.03162062\n",
      "Iteration 897, loss = 0.03155303\n",
      "Iteration 898, loss = 0.03144374\n",
      "Iteration 899, loss = 0.03133424\n",
      "Iteration 900, loss = 0.03127770\n",
      "Iteration 901, loss = 0.03118171\n",
      "Iteration 902, loss = 0.03109424\n",
      "Iteration 903, loss = 0.03104495\n",
      "Iteration 904, loss = 0.03094742\n",
      "Iteration 905, loss = 0.03081149\n",
      "Iteration 906, loss = 0.03072942\n",
      "Iteration 907, loss = 0.03065266\n",
      "Iteration 908, loss = 0.03054171\n",
      "Iteration 909, loss = 0.03050164\n",
      "Iteration 910, loss = 0.03038607\n",
      "Iteration 911, loss = 0.03030880\n",
      "Iteration 912, loss = 0.03021597\n",
      "Iteration 913, loss = 0.03014345\n",
      "Iteration 914, loss = 0.03003647\n",
      "Iteration 915, loss = 0.02996235\n",
      "Iteration 916, loss = 0.02988442\n",
      "Iteration 917, loss = 0.02976299\n",
      "Iteration 918, loss = 0.02970584\n",
      "Iteration 919, loss = 0.02960917\n",
      "Iteration 920, loss = 0.02951207\n",
      "Iteration 921, loss = 0.02947224\n",
      "Iteration 922, loss = 0.02937472\n",
      "Iteration 923, loss = 0.02925793\n",
      "Iteration 924, loss = 0.02918592\n",
      "Iteration 925, loss = 0.02914976\n",
      "Iteration 926, loss = 0.02904091\n",
      "Iteration 927, loss = 0.02894357\n",
      "Iteration 928, loss = 0.02888127\n",
      "Iteration 929, loss = 0.02881014\n",
      "Iteration 930, loss = 0.02870242\n",
      "Iteration 931, loss = 0.02860977\n",
      "Iteration 932, loss = 0.02854693\n",
      "Iteration 933, loss = 0.02848117\n",
      "Iteration 934, loss = 0.02837960\n",
      "Iteration 935, loss = 0.02831094\n",
      "Iteration 936, loss = 0.02827512\n",
      "Iteration 937, loss = 0.02818017\n",
      "Iteration 938, loss = 0.02819499\n",
      "Iteration 939, loss = 0.02802731\n",
      "Iteration 940, loss = 0.02793951\n",
      "Iteration 941, loss = 0.02781850\n",
      "Iteration 942, loss = 0.02773576\n",
      "Iteration 943, loss = 0.02768390\n",
      "Iteration 944, loss = 0.02759441\n",
      "Iteration 945, loss = 0.02749820\n",
      "Iteration 946, loss = 0.02744583\n",
      "Iteration 947, loss = 0.02745072\n",
      "Iteration 948, loss = 0.02732725\n",
      "Iteration 949, loss = 0.02724406\n",
      "Iteration 950, loss = 0.02716988\n",
      "Iteration 951, loss = 0.02707528\n",
      "Iteration 952, loss = 0.02698439\n",
      "Iteration 953, loss = 0.02690429\n",
      "Iteration 954, loss = 0.02684676\n",
      "Iteration 955, loss = 0.02675053\n",
      "Iteration 956, loss = 0.02667703\n",
      "Iteration 957, loss = 0.02665245\n",
      "Iteration 958, loss = 0.02654351\n",
      "Iteration 959, loss = 0.02649580\n",
      "Iteration 960, loss = 0.02647502\n",
      "Iteration 961, loss = 0.02634824\n",
      "Iteration 962, loss = 0.02625013\n",
      "Iteration 963, loss = 0.02618172\n",
      "Iteration 964, loss = 0.02610316\n",
      "Iteration 965, loss = 0.02606363\n",
      "Iteration 966, loss = 0.02596673\n",
      "Iteration 967, loss = 0.02596452\n",
      "Iteration 968, loss = 0.02583368\n",
      "Iteration 969, loss = 0.02577184\n",
      "Iteration 970, loss = 0.02568481\n",
      "Iteration 971, loss = 0.02562334\n",
      "Iteration 972, loss = 0.02555853\n",
      "Iteration 973, loss = 0.02544821\n",
      "Iteration 974, loss = 0.02536915\n",
      "Iteration 975, loss = 0.02532606\n",
      "Iteration 976, loss = 0.02526379\n",
      "Iteration 977, loss = 0.02517261\n",
      "Iteration 978, loss = 0.02508900\n",
      "Iteration 979, loss = 0.02505250\n",
      "Iteration 980, loss = 0.02500784\n",
      "Iteration 981, loss = 0.02488857\n",
      "Iteration 982, loss = 0.02484189\n",
      "Iteration 983, loss = 0.02476490\n",
      "Iteration 984, loss = 0.02470487\n",
      "Iteration 985, loss = 0.02459922\n",
      "Iteration 986, loss = 0.02455394\n",
      "Iteration 987, loss = 0.02451716\n",
      "Iteration 988, loss = 0.02443953\n",
      "Iteration 989, loss = 0.02436718\n",
      "Iteration 990, loss = 0.02433571\n",
      "Iteration 991, loss = 0.02421923\n",
      "Iteration 992, loss = 0.02416387\n",
      "Iteration 993, loss = 0.02409658\n",
      "Iteration 994, loss = 0.02408587\n",
      "Iteration 995, loss = 0.02398575\n",
      "Iteration 996, loss = 0.02389995\n",
      "Iteration 997, loss = 0.02381524\n",
      "Iteration 998, loss = 0.02376574\n",
      "Iteration 999, loss = 0.02373158\n",
      "Iteration 1000, loss = 0.02364611\n",
      "Iteration 1001, loss = 0.02367035\n",
      "Iteration 1002, loss = 0.02357907\n",
      "Iteration 1003, loss = 0.02345578\n",
      "Iteration 1004, loss = 0.02337433\n",
      "Iteration 1005, loss = 0.02333380\n",
      "Iteration 1006, loss = 0.02327669\n",
      "Iteration 1007, loss = 0.02321889\n",
      "Iteration 1008, loss = 0.02312959\n",
      "Iteration 1009, loss = 0.02309576\n",
      "Iteration 1010, loss = 0.02302121\n",
      "Iteration 1011, loss = 0.02295767\n",
      "Iteration 1012, loss = 0.02291060\n",
      "Iteration 1013, loss = 0.02282146\n",
      "Iteration 1014, loss = 0.02279414\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, verbose=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural_credit = MLPClassifier(max_iter=1500, \n",
    "                                   verbose=True, \n",
    "                                   tol=0.0001, \n",
    "                                   solver='adam',\n",
    "                                   activation='relu',\n",
    "                                   hidden_layer_sizes=(2,2))\n",
    "rede_neural_credit.fit(X_credit_train, y_credit_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = rede_neural_credit.predict(X_credit_test)\n",
    "previsoes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_credit_test, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(rede_neural_credit)\n",
    "cm.fit(X_credit_train, y_credit_train)\n",
    "cm.score(X_credit_test, y_credit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       0.98      1.00      0.99        64\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       0.99      1.00      1.00       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_credit_test,previsoes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('census.pkl', 'rb') as f:\n",
    "    X_census_train, y_census_train, X_census_test, y_census_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treino:  (27676, 108) (27676,)\n",
      "teste:  (4885, 108) (4885,)\n"
     ]
    }
   ],
   "source": [
    "print('treino: ',X_census_train.shape, y_census_train.shape)\n",
    "print('teste: ',X_census_test.shape, y_census_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.38987679\n",
      "Iteration 2, loss = 0.32477977\n",
      "Iteration 3, loss = 0.31322680\n",
      "Iteration 4, loss = 0.30572784\n",
      "Iteration 5, loss = 0.30099045\n",
      "Iteration 6, loss = 0.29682460\n",
      "Iteration 7, loss = 0.29358076\n",
      "Iteration 8, loss = 0.29189157\n",
      "Iteration 9, loss = 0.28845301\n",
      "Iteration 10, loss = 0.28719561\n",
      "Iteration 11, loss = 0.28441797\n",
      "Iteration 12, loss = 0.28214083\n",
      "Iteration 13, loss = 0.28047818\n",
      "Iteration 14, loss = 0.27834034\n",
      "Iteration 15, loss = 0.27667193\n",
      "Iteration 16, loss = 0.27484657\n",
      "Iteration 17, loss = 0.27359410\n",
      "Iteration 18, loss = 0.27140404\n",
      "Iteration 19, loss = 0.26963489\n",
      "Iteration 20, loss = 0.26815090\n",
      "Iteration 21, loss = 0.26638066\n",
      "Iteration 22, loss = 0.26531763\n",
      "Iteration 23, loss = 0.26331625\n",
      "Iteration 24, loss = 0.26139815\n",
      "Iteration 25, loss = 0.26011495\n",
      "Iteration 26, loss = 0.25890124\n",
      "Iteration 27, loss = 0.25746230\n",
      "Iteration 28, loss = 0.25564138\n",
      "Iteration 29, loss = 0.25517164\n",
      "Iteration 30, loss = 0.25394986\n",
      "Iteration 31, loss = 0.25246126\n",
      "Iteration 32, loss = 0.25032654\n",
      "Iteration 33, loss = 0.24916821\n",
      "Iteration 34, loss = 0.24870277\n",
      "Iteration 35, loss = 0.24637188\n",
      "Iteration 36, loss = 0.24595990\n",
      "Iteration 37, loss = 0.24441065\n",
      "Iteration 38, loss = 0.24340388\n",
      "Iteration 39, loss = 0.24234160\n",
      "Iteration 40, loss = 0.24101823\n",
      "Iteration 41, loss = 0.24018403\n",
      "Iteration 42, loss = 0.23988339\n",
      "Iteration 43, loss = 0.23897515\n",
      "Iteration 44, loss = 0.23839819\n",
      "Iteration 45, loss = 0.23627208\n",
      "Iteration 46, loss = 0.23470658\n",
      "Iteration 47, loss = 0.23458895\n",
      "Iteration 48, loss = 0.23374561\n",
      "Iteration 49, loss = 0.23319025\n",
      "Iteration 50, loss = 0.23089147\n",
      "Iteration 51, loss = 0.22961750\n",
      "Iteration 52, loss = 0.22984619\n",
      "Iteration 53, loss = 0.22845555\n",
      "Iteration 54, loss = 0.22799667\n",
      "Iteration 55, loss = 0.22765389\n",
      "Iteration 56, loss = 0.22637511\n",
      "Iteration 57, loss = 0.22559378\n",
      "Iteration 58, loss = 0.22476040\n",
      "Iteration 59, loss = 0.22337651\n",
      "Iteration 60, loss = 0.22368354\n",
      "Iteration 61, loss = 0.22255742\n",
      "Iteration 62, loss = 0.22133035\n",
      "Iteration 63, loss = 0.22081326\n",
      "Iteration 64, loss = 0.21962051\n",
      "Iteration 65, loss = 0.21834573\n",
      "Iteration 66, loss = 0.21796000\n",
      "Iteration 67, loss = 0.21708310\n",
      "Iteration 68, loss = 0.21756363\n",
      "Iteration 69, loss = 0.21604211\n",
      "Iteration 70, loss = 0.21648659\n",
      "Iteration 71, loss = 0.21529326\n",
      "Iteration 72, loss = 0.21458704\n",
      "Iteration 73, loss = 0.21274934\n",
      "Iteration 74, loss = 0.21340859\n",
      "Iteration 75, loss = 0.21138513\n",
      "Iteration 76, loss = 0.21114917\n",
      "Iteration 77, loss = 0.21018528\n",
      "Iteration 78, loss = 0.21012597\n",
      "Iteration 79, loss = 0.20958437\n",
      "Iteration 80, loss = 0.20927543\n",
      "Iteration 81, loss = 0.20761863\n",
      "Iteration 82, loss = 0.20724769\n",
      "Iteration 83, loss = 0.20822755\n",
      "Iteration 84, loss = 0.20831931\n",
      "Iteration 85, loss = 0.20467299\n",
      "Iteration 86, loss = 0.20585632\n",
      "Iteration 87, loss = 0.20458223\n",
      "Iteration 88, loss = 0.20426224\n",
      "Iteration 89, loss = 0.20273682\n",
      "Iteration 90, loss = 0.20319207\n",
      "Iteration 91, loss = 0.20244776\n",
      "Iteration 92, loss = 0.20081323\n",
      "Iteration 93, loss = 0.20072036\n",
      "Iteration 94, loss = 0.20070168\n",
      "Iteration 95, loss = 0.20000969\n",
      "Iteration 96, loss = 0.19955959\n",
      "Iteration 97, loss = 0.19813520\n",
      "Iteration 98, loss = 0.19791292\n",
      "Iteration 99, loss = 0.19788479\n",
      "Iteration 100, loss = 0.19745079\n",
      "Iteration 101, loss = 0.19790361\n",
      "Iteration 102, loss = 0.19659440\n",
      "Iteration 103, loss = 0.19658635\n",
      "Iteration 104, loss = 0.19568697\n",
      "Iteration 105, loss = 0.19487296\n",
      "Iteration 106, loss = 0.19574403\n",
      "Iteration 107, loss = 0.19485182\n",
      "Iteration 108, loss = 0.19362521\n",
      "Iteration 109, loss = 0.19408582\n",
      "Iteration 110, loss = 0.19150510\n",
      "Iteration 111, loss = 0.19069406\n",
      "Iteration 112, loss = 0.19124459\n",
      "Iteration 113, loss = 0.19214239\n",
      "Iteration 114, loss = 0.19094518\n",
      "Iteration 115, loss = 0.19269632\n",
      "Iteration 116, loss = 0.18974622\n",
      "Iteration 117, loss = 0.18990240\n",
      "Iteration 118, loss = 0.18887737\n",
      "Iteration 119, loss = 0.18846588\n",
      "Iteration 120, loss = 0.18830586\n",
      "Iteration 121, loss = 0.18809823\n",
      "Iteration 122, loss = 0.18756730\n",
      "Iteration 123, loss = 0.18774814\n",
      "Iteration 124, loss = 0.18698441\n",
      "Iteration 125, loss = 0.18590037\n",
      "Iteration 126, loss = 0.18688776\n",
      "Iteration 127, loss = 0.18559078\n",
      "Iteration 128, loss = 0.18572207\n",
      "Iteration 129, loss = 0.18561560\n",
      "Iteration 130, loss = 0.18410678\n",
      "Iteration 131, loss = 0.18343365\n",
      "Iteration 132, loss = 0.18382545\n",
      "Iteration 133, loss = 0.18451482\n",
      "Iteration 134, loss = 0.18233248\n",
      "Iteration 135, loss = 0.18276167\n",
      "Iteration 136, loss = 0.18291647\n",
      "Iteration 137, loss = 0.18268986\n",
      "Iteration 138, loss = 0.18285960\n",
      "Iteration 139, loss = 0.18135131\n",
      "Iteration 140, loss = 0.18213102\n",
      "Iteration 141, loss = 0.18088368\n",
      "Iteration 142, loss = 0.17984033\n",
      "Iteration 143, loss = 0.17966878\n",
      "Iteration 144, loss = 0.18031011\n",
      "Iteration 145, loss = 0.18024788\n",
      "Iteration 146, loss = 0.18125577\n",
      "Iteration 147, loss = 0.17945388\n",
      "Iteration 148, loss = 0.17775229\n",
      "Iteration 149, loss = 0.17815794\n",
      "Iteration 150, loss = 0.17687928\n",
      "Iteration 151, loss = 0.17742755\n",
      "Iteration 152, loss = 0.17801582\n",
      "Iteration 153, loss = 0.17711296\n",
      "Iteration 154, loss = 0.17732096\n",
      "Iteration 155, loss = 0.17590703\n",
      "Iteration 156, loss = 0.17573624\n",
      "Iteration 157, loss = 0.17759995\n",
      "Iteration 158, loss = 0.17555840\n",
      "Iteration 159, loss = 0.17618311\n",
      "Iteration 160, loss = 0.17514470\n",
      "Iteration 161, loss = 0.17475940\n",
      "Iteration 162, loss = 0.17616692\n",
      "Iteration 163, loss = 0.17363471\n",
      "Iteration 164, loss = 0.17432859\n",
      "Iteration 165, loss = 0.17472211\n",
      "Iteration 166, loss = 0.17311438\n",
      "Iteration 167, loss = 0.17277329\n",
      "Iteration 168, loss = 0.17473142\n",
      "Iteration 169, loss = 0.17149580\n",
      "Iteration 170, loss = 0.17251603\n",
      "Iteration 171, loss = 0.17182470\n",
      "Iteration 172, loss = 0.17310513\n",
      "Iteration 173, loss = 0.17212860\n",
      "Iteration 174, loss = 0.17299189\n",
      "Iteration 175, loss = 0.17232159\n",
      "Iteration 176, loss = 0.16890795\n",
      "Iteration 177, loss = 0.17029637\n",
      "Iteration 178, loss = 0.16914609\n",
      "Iteration 179, loss = 0.17032072\n",
      "Iteration 180, loss = 0.16887866\n",
      "Iteration 181, loss = 0.16840289\n",
      "Iteration 182, loss = 0.16959266\n",
      "Iteration 183, loss = 0.16988804\n",
      "Iteration 184, loss = 0.16809050\n",
      "Iteration 185, loss = 0.16923738\n",
      "Iteration 186, loss = 0.16887219\n",
      "Iteration 187, loss = 0.16969630\n",
      "Iteration 188, loss = 0.17019470\n",
      "Iteration 189, loss = 0.16727692\n",
      "Iteration 190, loss = 0.16615286\n",
      "Iteration 191, loss = 0.16724970\n",
      "Iteration 192, loss = 0.16794846\n",
      "Iteration 193, loss = 0.16753512\n",
      "Iteration 194, loss = 0.16654938\n",
      "Iteration 195, loss = 0.16885598\n",
      "Iteration 196, loss = 0.16818928\n",
      "Iteration 197, loss = 0.16700588\n",
      "Iteration 198, loss = 0.16505049\n",
      "Iteration 199, loss = 0.16647396\n",
      "Iteration 200, loss = 0.16603164\n",
      "Iteration 201, loss = 0.16614505\n",
      "Iteration 202, loss = 0.16544450\n",
      "Iteration 203, loss = 0.16474039\n",
      "Iteration 204, loss = 0.16341383\n",
      "Iteration 205, loss = 0.16391736\n",
      "Iteration 206, loss = 0.16394915\n",
      "Iteration 207, loss = 0.16477678\n",
      "Iteration 208, loss = 0.16369171\n",
      "Iteration 209, loss = 0.16394257\n",
      "Iteration 210, loss = 0.16338406\n",
      "Iteration 211, loss = 0.16308119\n",
      "Iteration 212, loss = 0.16356201\n",
      "Iteration 213, loss = 0.16218295\n",
      "Iteration 214, loss = 0.16214466\n",
      "Iteration 215, loss = 0.16196535\n",
      "Iteration 216, loss = 0.16269368\n",
      "Iteration 217, loss = 0.16206466\n",
      "Iteration 218, loss = 0.16255910\n",
      "Iteration 219, loss = 0.16068654\n",
      "Iteration 220, loss = 0.16012335\n",
      "Iteration 221, loss = 0.15974083\n",
      "Iteration 222, loss = 0.16260896\n",
      "Iteration 223, loss = 0.16111311\n",
      "Iteration 224, loss = 0.16031298\n",
      "Iteration 225, loss = 0.15951333\n",
      "Iteration 226, loss = 0.16028023\n",
      "Iteration 227, loss = 0.15908465\n",
      "Iteration 228, loss = 0.16156083\n",
      "Iteration 229, loss = 0.15947924\n",
      "Iteration 230, loss = 0.15866297\n",
      "Iteration 231, loss = 0.15880135\n",
      "Iteration 232, loss = 0.15953656\n",
      "Iteration 233, loss = 0.15818240\n",
      "Iteration 234, loss = 0.15832266\n",
      "Iteration 235, loss = 0.15861712\n",
      "Iteration 236, loss = 0.15860238\n",
      "Iteration 237, loss = 0.15724807\n",
      "Iteration 238, loss = 0.15895547\n",
      "Iteration 239, loss = 0.15696811\n",
      "Iteration 240, loss = 0.15701706\n",
      "Iteration 241, loss = 0.15744081\n",
      "Iteration 242, loss = 0.15739409\n",
      "Iteration 243, loss = 0.15630667\n",
      "Iteration 244, loss = 0.15720982\n",
      "Iteration 245, loss = 0.15636974\n",
      "Iteration 246, loss = 0.15591054\n",
      "Iteration 247, loss = 0.15516167\n",
      "Iteration 248, loss = 0.15589612\n",
      "Iteration 249, loss = 0.15631147\n",
      "Iteration 250, loss = 0.15736230\n",
      "Iteration 251, loss = 0.15525501\n",
      "Iteration 252, loss = 0.15561399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.15614452\n",
      "Iteration 254, loss = 0.15777476\n",
      "Iteration 255, loss = 0.15439704\n",
      "Iteration 256, loss = 0.15405987\n",
      "Iteration 257, loss = 0.15456423\n",
      "Iteration 258, loss = 0.15552872\n",
      "Iteration 259, loss = 0.15346461\n",
      "Iteration 260, loss = 0.15470047\n",
      "Iteration 261, loss = 0.15428808\n",
      "Iteration 262, loss = 0.15371533\n",
      "Iteration 263, loss = 0.15391316\n",
      "Iteration 264, loss = 0.15498862\n",
      "Iteration 265, loss = 0.15322426\n",
      "Iteration 266, loss = 0.15457745\n",
      "Iteration 267, loss = 0.15193113\n",
      "Iteration 268, loss = 0.15195866\n",
      "Iteration 269, loss = 0.15377725\n",
      "Iteration 270, loss = 0.15359819\n",
      "Iteration 271, loss = 0.15052211\n",
      "Iteration 272, loss = 0.15487692\n",
      "Iteration 273, loss = 0.15274740\n",
      "Iteration 274, loss = 0.15111903\n",
      "Iteration 275, loss = 0.15125358\n",
      "Iteration 276, loss = 0.15226108\n",
      "Iteration 277, loss = 0.15246525\n",
      "Iteration 278, loss = 0.15086817\n",
      "Iteration 279, loss = 0.15030384\n",
      "Iteration 280, loss = 0.14971833\n",
      "Iteration 281, loss = 0.15314194\n",
      "Iteration 282, loss = 0.15078432\n",
      "Iteration 283, loss = 0.15055030\n",
      "Iteration 284, loss = 0.15075033\n",
      "Iteration 285, loss = 0.15100331\n",
      "Iteration 286, loss = 0.15076937\n",
      "Iteration 287, loss = 0.14982806\n",
      "Iteration 288, loss = 0.14960321\n",
      "Iteration 289, loss = 0.15051583\n",
      "Iteration 290, loss = 0.14783996\n",
      "Iteration 291, loss = 0.14991993\n",
      "Iteration 292, loss = 0.14837172\n",
      "Iteration 293, loss = 0.14849019\n",
      "Iteration 294, loss = 0.14985827\n",
      "Iteration 295, loss = 0.14854882\n",
      "Iteration 296, loss = 0.14834125\n",
      "Iteration 297, loss = 0.14899864\n",
      "Iteration 298, loss = 0.14821224\n",
      "Iteration 299, loss = 0.14702304\n",
      "Iteration 300, loss = 0.14731491\n",
      "Iteration 301, loss = 0.14839318\n",
      "Iteration 302, loss = 0.14581729\n",
      "Iteration 303, loss = 0.14710945\n",
      "Iteration 304, loss = 0.14713283\n",
      "Iteration 305, loss = 0.14803868\n",
      "Iteration 306, loss = 0.14724887\n",
      "Iteration 307, loss = 0.14946901\n",
      "Iteration 308, loss = 0.14749126\n",
      "Iteration 309, loss = 0.14820189\n",
      "Iteration 310, loss = 0.14829815\n",
      "Iteration 311, loss = 0.14723273\n",
      "Iteration 312, loss = 0.14612274\n",
      "Iteration 313, loss = 0.14726930\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural_census = MLPClassifier(max_iter=1000, \n",
    "                                   verbose=True, \n",
    "                                   tol=0.00001, \n",
    "                                   solver='adam',\n",
    "                                   activation='relu',\n",
    "                                   hidden_layer_sizes=(55,55))\n",
    "rede_neural_census.fit(X_census_train, y_census_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4885,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = rede_neural_census.predict(X_census_test)\n",
    "previsoes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8212896622313204"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(rede_neural_census)\n",
    "cm.fit(X_census_train, y_census_train)\n",
    "cm.score(X_census_test, y_census_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.89      0.88      3693\n",
      "        >50K       0.64      0.61      0.62      1192\n",
      "\n",
      "    accuracy                           0.82      4885\n",
      "   macro avg       0.76      0.75      0.75      4885\n",
      "weighted avg       0.82      0.82      0.82      4885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_census_test,previsoes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
